---
title: '<center> The Exercises of Week 1 <center>'
author: '<center> Reza Mohammadi <center>'
date: '<center> `r Sys.Date()` <center>'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 5
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include = FALSE}
knitr::opts_chunk $ set( echo = TRUE, message = FALSE, warning = FALSE, 
                         comment = " ", error = FALSE, fig.align = 'center'  )
```

```{css, echo=FALSE}
.answer_panel {
  margin-left: 40px;
}
```

**Full Name:**

1. Peter Molnar (11759216)
2. Lourenço Santos Moitinho de Almeida (12636622)

**Online Exercises on DataCamp**: 
Register at the [DataCamp](https://www.datacamp.com) for the free online course “Data Analytics 2022”. Three online assignments are waiting for you. Those online assignments are preparing you for the computer labs. *The online assignments at the DataCamp are not mandatory*.

**Your task** is to answer the questions in this R-markdown file. Submit both your R-markdown (.Rmd) file and the HTML file on Canvas. Note that your R-markdown has to be in the right format. 

# How to use R-Markdown in RStudio?

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

For a quick demo on how to use R Markdown in RStudio see: [https://www.youtube.com/embed/DNS7i2m4sB0](https://www.youtube.com/embed/DNS7i2m4sB0)

Download the Rmd file with the name ‘Exercises_week1.Rmd’ at [Canvas > Week1 - Introduction to Data Science](https://canvas.uva.nl/courses/32529/assignments/336183?module_item_id=1378167) and save it in your personal computer. Open this Rmd file on your RStudio and create its HTML file by clicking on the Work space Tab on `Knit > Knit to HTML`.

# Loading/Attaching **R** packages

Sometimes we need some function/dataset which is not in **R**. In this case, we need to install and load the package that includes that function/dataset. Note, we need to install the package once; the next time that we open **R**, we only need to load the package. For example, here, we need to load the following R packages:

* [**ggplot2**](https://CRAN.R-project.org/package=ggplot2): we use the functionality of this package for data visualization.
* [**plyr**](https://CRAN.R-project.org/package=plyr): for function 'mutate'.
* [**Hmisc**](https://CRAN.R-project.org/package=Hmisc): for missing vlaues.
* [**liver**](https://CRAN.R-project.org/package=liver): the *adult* dataset is in this package. 

We import the following packages in R as follows:
```{r message = FALSE, warning = FALSE}
library( ggplot2 )
library( plyr    )  # for function 'mutate'.
library( liver   )

library( forcats )  # for function "fct_collapse"

library( Hmisc   )  # for missing values
library( naniar  )  # for visualizing the missing values
```

**NOTE:** If you have not installed these packages, you should first install them.

So, if it is needed, install the packages. In RStudio, after clicking on the "Tools" tab, click on "Install Packages...". In the Install Packages dialog, write the package name you want to install under the Packages field and then click install. **NOTE** Please select the option "Install dependencies".

**Your task here is to install these packages on our computer.**

# Diamonds Dataset
 
Here we want to use the *diamonds* dataset as an example of how to deal with *unusual values*, *outliers*, and *missing values*. The *diamonds* dataset is available in the R package [**ggplot2**](https://CRAN.R-project.org/package=ggplot2). 

In general, we could import the Dataset sheet from our personal computer or an online source into **R**, by using the `read.csv()` function. But, here, since the *diamonds* dataset is available in the **R** package "*ggplot2*", we import the *diamonds* dataset in **R** as follows:
```{r}
data( diamonds ) # loads "diamonds" data in your RStudio environment
``` 

To see the overview of the dataset in **R**, we could use the following functions: 

* `str()`  to see a compact display of the structure of the data. 
* `View()` to see spreadsheet-style data. 
* `head()` to see the first part of the data (first 6-rows of data).
* `summary()` to see the summary of each variable.

Here we use the `str()` function to report the structure of the *diamonds* dataset as follows
```{r}
str( diamonds )   
```

It shows the dataset has `r nrow( diamonds )` observations and `r ncol( diamonds )` variables where:

* `price`: price in US dollars (\$326–\$18,823).
* `carat`: weight of the diamond (0.2–5.01).
* `cut`: quality of the cut (Fair, Good, Very Good, Premium, Ideal).
* `color`: diamond colour, from D (best) to J (worst).
* `clarity`: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)).
* `x`: length in mm (0–10.74).
* `y`: width in mm (0–58.9).
* `z`: depth in mm (0–31.8).
* `depth`: total depth percentage = `z` / mean(`x`, `y`) = 2 * `z` / (`x` + `y`).

To see the first part of the data
```{r}
head( diamonds )   
```

You can find more information related to this dataset [here](https://ggplot2.tidyverse.org/reference/diamonds.html). 

a. **Which of the variables in the dataset are *nominal*? Which ones are *Ordinal*? Which ones are *numerical*?**

<details class="answer_panel">
  <summary>Answer a</summary>
* nominal variables: The dataset does not contain any nominal variables.
* ordinal variables: `cut`, `color`, `clarity`
* numerical variables: `carat`, `depth`, `table`, `price`, `x`, `y`, `z`
</details> 

b. **Report the summary of the *diamonds* data? set by using function `summary()`.**

<details class="answer_panel">
  <summary>Answer b</summary>
```{r}
summary(diamonds)
```
</details> 

c. **Report the average, minimum, and maximum price for the diamonds.**

<details class="answer_panel">
  <summary>Answer c</summary>
```{r}
mean_price <- mean(diamonds[["price"]])
```

The average price of a diamond in the `diamonds` dataset is $`r  round(mean_price, digits = 3)`.

```{r}
min_price <- min(diamonds["price"])
```

The cheapest diamond of the `diamonds` dataset is $`r  min_price`.

```{r}
max_price <- max(diamonds["price"])
```
The most expensive diamond of the `diamonds` dataset is $`r  max_price`.

</details>

## Unusual Values in the *diamonds* dataset 

Outliers are observations that are unusual; data points that don't seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important information. When you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the `y` variable from the *diamonds* dataset. The only evidence of outliers is the unusually wide limits on the y-axis:

```{r fig.align = 'center'}
ggplot( diamonds ) +
    geom_histogram( mapping = aes( x = y ), binwidth = 0.5 )
```

There are so many observations in the common bins that the rare bins are so short that you cann't see them (although maybe if you stare intently at 0 you'll spot something). To make it easy to see the unusual values, we need to zoom in to small values of the y-axis:

```{r fig.align = 'center'}
ggplot( diamonds ) +
    geom_histogram( mapping = aes( x = y ), binwidth = 0.5 ) +
    coord_cartesian( ylim = c( 0, 30 ) )
```

We also report the scatter plot for variable `x` vs `price`:
```{r fig.align = 'center'}
ggplot( data = diamonds, mapping = aes( x = y, y = price ) ) + 
  geom_point( colour = 'blue' )
```

The `y` variable measures one of the three dimensions of these diamonds, in mm. We know that diamonds can’t have a width of 0mm, so these values must be incorrect. We might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars!

It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.

I recommend replacing the unusual values with missing values. The easiest way to do this is to use `mutate()` to replace the variable with a modified copy. You can use the `ifelse()` function to replace unusual values with `NA`:

```{r}
diamonds_2 = mutate( diamonds, y = ifelse( y ==  0 | y > 30, NA, y ) ) 
```

[`ifelse()`](https://rdrr.io/r/base/ifelse.html) has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, `yes`, when test is `TRUE`, and the value of the third argument, `no`, when it is `FALSE`. 

```{r fig.align = 'center'}
ggplot( data = diamonds_2, mapping = aes( x = y, y = price ) ) + 
  geom_point( colour = 'blue' )
```

To see the summary of the variable `y` with missing values:
```{r}
summary( diamonds_2 )
```

## Dealing with missing values

Here we impute the missing values with *random* values (which is proportional to categories' records) by using the function `impute()` from R package **Hmisc** as follows: 
```{r}
diamonds_2 $ y = impute( diamonds_2 $ y, 'random' )
```

To see the summary of the variable `y`:
```{r}
summary( diamonds_2 $ y )
```

## Unusual Values in the variables `x` and `z`

Here, in the *diamonds* dataset, we want to check if there is any unusual or outliers in the variables `x` and `z`. 

**Your task here is to check if there is any unusual or outliers in the variables `x` and `z`. Follow the same steps at above for the variable `y`.**

<details class="answer_panel">
  <summary>Answer</summary>
  
  <h3>Outliers in `x`</h3>

To see if variable `x` contains any outliers, first we are going to plot it as a histogram.

```{r}
ggplot( diamonds_2 ) +
    geom_histogram( mapping = aes( x = x ), binwidth = 0.5 )
```

```{r echo=FALSE}
min_x = min(diamonds_2 $ x)
max_x = max(diamonds_2 $ x)
```

The limits of the x-axis does not have a wide limit, therefore, this does not necessarily refer to outliers. The observations spread out within a range of [`r min_x`, `r max_x`]. 

By looking at the two extreme values of `x`, its minimum is at `r min_x`mm, and maximum at `r max_x`mm. We can already rule out 0mm, since a diamond's width cannot be 0mm. It, however, requires further investigation to understand how the data were recorded, and whether 0mm is a measurement error, or a codified indication of a missing value.

The other extreme of the range is `r max_x`mm, and it also requires investigation to be classified as an outlier or not. For that we plot variable `x` against `price` to see whether the unusual width of the diamond is justified by its price.

```{r}
ggplot( data = diamonds_2, mapping = aes( x = x, y = price ) ) + 
  geom_point( colour = 'blue' )
```

The above scatterplot shows, that diamonds with length above 9mm tend to deviate from the bulk of the data. We also see that although having an unusual length of diamond, it does not sell for a proportionately higher price. Since variable `x` is just one of many explanatory variables that may explain `price`, other variables such as cut, colour or clarity may very well have a significant explanatory power of  price. Therefore, judging the dimensions of a diamond isolatively with price does not give a valid criteria of what counts as an outlier.

Based on the above argumentation, we believe the only unequivocal outliers are of width 0mm, therefore we replace them with with missing values, and plot the new data.

```{r}
diamonds_3 = mutate( diamonds_2, x = ifelse( x == 0, NA, x) )

ggplot( data = diamonds_3, mapping = aes( x = x, y = price ) ) + 
  geom_point( colour = 'blue' )
```

Below we report a summary of variable `x` with the new missing values present in the dataset.

```{r}
summary(diamonds_3 $ x)
```

Per above summary we see that 8 observations were replaced by NA values. Next, we replace the missing values with random values proportional to the category's records.

```{r}
diamonds_3 $ x = impute( diamonds_3 $ x, 'random' )
```

Finally, we report the imputed summary for `x`:

```{r}
summary( diamonds_3 $ x )
```

  <h3>Outliers in `z`</h3>
  
Next, we move on to the exploration of `z`. First, we plot `z` as a histogram and look for anomalies, and observations that are far from the bulk of the data.

```{r}
ggplot( diamonds_3 ) +
    geom_histogram( mapping = aes( x = z ), binwidth = 0.5 )
```

```{r echo=FALSE}
min_z = min(diamonds $ z)
max_z = max(diamonds $ z)
```

The observations along the x-axis are widely spread out within the range of [`r min_z`, `r max_z`], which is an unofficial indication of outliers present in `z`. By looking at the two extreme values of `z`, the minimum of `r min_z`mm, can already be deemed an outlier, since a diamond's depth cannot be 0mm.

Without knowing the data recording procedure or methodology, we cannot conclude whether 0mm is a measurement error, or a codified indication of a missing value.

To make the unusual values along the x-axis more visible, we adjusted the limits of the y-axis, so that it ranges between 0 and 30 counts.

```{r}
ggplot( diamonds_3 ) +
    geom_histogram( mapping = aes( x = z ), binwidth = 0.5 ) +
    coord_cartesian( ylim = c( 0, 30 ) )
```

We see the `r max_z`mm greatly deviates from the rest of the data. In order to determine if the observation with a depth over 30mm is an outlier we plot `z` against `price` in a scatterplot.

```{r}
ggplot( data = diamonds_3, mapping = aes( x = z, y = price ) ) + 
  geom_point( colour = 'blue' )
```

The above histogram clearly shows that the largest diamond in terms of depth sells for similar prices as those of 3-5mm. Therefore, it is likely, that the decimal point of 31.8mm was misplaced when recording the data and was meant to be 3.18mm instead. Nevertheless, such observation is numerically so distant from the rest of the data that it can be deemed an outlier.

Therefore, we replace the above classified unusual values with NA, plot the new data, and report a summary of the replaced values.

```{r}
diamonds_4 = mutate( diamonds_3, z = ifelse( z == 0 | z > 10, NA, z) )

ggplot( data = diamonds_4, mapping = aes( x = z, y = price ) ) + 
  geom_point( colour = 'blue' )

summary(diamonds_4 $ z)
```

We see, that in total 21 outliers were replaced by NA values. Next, we impute the new missing values with random values that are proportional to the category's records, and report a summary on the imputed `z` variable. 

```{r}
diamonds_4 $ z = impute( diamonds_4 $ z, 'random' )
summary(diamonds_4 $ z)
```

</details>

# Adult Dataset

We want to find out *which type of people can earn more than 50K per year*. Thus, the prediction task is to determine whether a person makes over $50K a year. For this classification task, in week 5, we will apply the *Decision Tree* as well as the *Random Forest* algorithm. 

## Data Understanding

You could find more information about this dataset here: [https://rdrr.io/cran/liver/man/adult.html](https://rdrr.io/cran/liver/man/adult.html). 

The *adult* dataset is from the [US Census Bureau](https://www.census.gov) with the primary task to predict whether a given adult makes more than $50K a year based on attributes such as education, hours of work per week, etc. The target feature is *income* with two levels "<=50K" and ">50K", and the remaining 14 variables are predictors.

The *adult* dataset, as a data frame, contains 48598 records (rows) with 15 variables/features (columns):

* `age`: age in years (numerical).
* `workclass`: a factor with 6 levels (categorical-nominal). 
* `demogweight`: the demographics to describe a person (categorical-nominal).
* `education`: a factor with 16 levels (categorical-nominal).
* `education.num`: number of years of education (numerical-discrete).
* `marital.status`: a factor with 5 levels (categorical-nominal).
* `occupation`: a factor with 15 levels (categorical-nominal).
* `relationship`: a factor with 6 levels (categorical-nominal).
* `race`: a factor with 5 levels (categorical-nominal).
* `gender`: a factor with levels "Female","Male" (categorical-binary).
* `capital.gain`: capital gains  (numerical-discrete).
* `capital.loss`: capital losses  (numerical-discrete).
* `hours.per.week`: number of hours of work per week (numerical-discrete).
* `native.country`: a factor with 42 levels (categorical-nominal).
* `income`: yearly income as a factor with levels "<=50K" and ">50K" (categorical-binary).

You can find more information related to this dataset at:

[https://www.rdocumentation.org/packages/liver/versions/1.3/topics/adult](https://www.rdocumentation.org/packages/liver/versions/1.3/topics/adult)

We import the dataset and report the structure of the dataset:

```{r}
data( adult ) 

str( adult )
```

It shows the dataset contains `r nrow( adult )` records and `r ncol( adult )` variables/features. The dataset has `r ncol( adult ) - 1` predictors along with a target variable `income` as a binary variable with two levels "<=50K" and ">50K".

a. **Which of the variables in the dataset are *nominal*? And which ones are *numerical*?**

<details class="answer_panel">
  <summary>Answer a</summary>
  
* nominal variables: `workclass`, `demogweight`, `marital.status`, `occupation`, `relationship`, `race`, `native.country`
* numerical variables: `age`, `education.num`, `capital.gain`, `capital.loss`, `hours.per.week`

</details>

b. **Report the summary of the *adult* dataset by using function `summary()`.**

<details class="answer_panel">
  <summary>Answer b</summary>
```{r}
summary(adult)
```

</details>

c. **Report the proportion of the people who earn more than 50K (`>50K`).**

<details class="answer_panel">
  <summary>Answer c</summary>
```{r}

income = adult[["income"]]

# to inspect the underlying integer values of the factor levels
levels(income)
```

The above output shows, that "<=50K" is associated with an underlying integer value `1`, and ">50K" with `2`. Although in this format the data is already binary in a sense that it can only take two mutually exclusive values, but is not binary in its literal sense. That is, its range is not within the boundaries of [0, 1], instead [1, 2]. If we were to take the mean of `income` now, it would result in an average 1 greater than the mean of a strictly binary variable of [0, 1]. To work with binary values, we need to convert the factor to values between 0 and 1. To achieve this, we take away 1 from each observations

```{r}
# take factor values as numeric
numeric_income = as.numeric(income)

# take away one from each value to bind the outcome to [0, 1]
numeric_income = numeric_income - 1

mean_num_income = mean(numeric_income)
```

According to the above calculation, `r round(mean_num_income * 100, 2)`% of the adults earn more than $50.000 annually.

</details>

## Data Cleaning

In the dataset, some of the categorical variables (`native.county` and `workclass`) have a lot of categories. Here, we reduce the number of factors. 

Using `table()` for checking the frequency of the native country column.
```{r}
table( adult $ native.country )
```

We will group these countries together into continents by using function `fct_collapse`.
```{r}
Europe = c( "England", "France", "Germany", "Greece", "Holand-Netherlands", 
            "Hungary", "Ireland", "Italy", "Poland", "Portugal", "Scotland", 
            "Yugoslavia" )

Asia = c( "China", "Hong", "India", "Iran", "Cambodia", "Japan", "Laos", 
          "Philippines", "Vietnam", "Taiwan", "Thailand" )

N.America = c( "Canada", "United-States", "Puerto-Rico" )

S.America = c( "Columbia", "Cuba", "Dominican-Republic", "Ecuador", "El-Salvador", 
             "Guatemala", "Haiti", "Honduras", "Mexico", "Nicaragua", 
             "Outlying-US(Guam-USVI-etc)", "Peru", "Jamaica", "Trinadad&Tobago" )

Other = c( "South" )

adult $ native.country = fct_collapse( adult $ native.country, "Europe"    = Europe    )
adult $ native.country = fct_collapse( adult $ native.country, "Asia"      = Asia      )
adult $ native.country = fct_collapse( adult $ native.country, "N.America" = N.America )
adult $ native.country = fct_collapse( adult $ native.country, "S.America" = S.America )
adult $ native.country = fct_collapse( adult $ native.country, "Other"     = Other     )
```

Using `table()` for checking the frequency of the native country column.
```{r}
table( adult $ native.country )
```

For the variable `workclass`, we report the frequency of workclass as
```{r}
table( adult $ workclass )
```

We reduce two categories "Never-worked" and "Without-pay" to "Unemployed" as follows:
```{r}
adult $ workclass = fct_collapse( adult $ workclass, "Unemployed" = c( "Never-worked", "Without-pay" ) )
```

## Missing Values

We can see from the output of the `summary()` function that variable `workclass` has a category of **?** with `r sum( adult$workclass=="?" )` records. The variable `native.country` also has a category of **?** with `r sum( adult$native.country=="?" )` records. Since these **?** are the missing values in the dataset, we convert them to `NA` values as follows:
```{r message = FALSE }
adult[ adult == "?" ] = NA
```

To remove the category "`?`"
```{r message = FALSE }
adult $ workclass      = factor( adult $ workclass     , levels = levels( adult $ workclass      )[ -1 ] )
adult $ native.country = factor( adult $ native.country, levels = levels( adult $ native.country )[ -1 ] )
adult $ occupation     = factor( adult $ occupation    , levels = levels( adult $ occupation     )[ -1 ] )
```


To visualize the missing values
```{r fig.align = 'center'}
gg_miss_var( adult, show_pct = TRUE )
```

The above plot shows that the variables `workclass`, `occupation`, and `native.country` have missing values. It shows that the variables `workclass`, `occupation`, and `native.country` have around less than 0.06 percent missing values and the variable `native.country` has around 0.02 percent missing values. 

Here we impute the missing values with *random* values (which is proportional to categories' records) by using the function `impute()` from R package **Hmisc** as follows: 
```{r}
# impute missing values with random value
adult $ workclass      = impute( adult $ workclass     , 'random' )
adult $ native.country = impute( adult $ native.country, 'random' ) 
adult $ occupation     = impute( adult $ occupation    , 'random' ) 
```

The below plot shows that all the missing values are imputed.
```{r fig.align = 'center'}
gg_miss_var( adult, show_pct = TRUE )
```

The `impute()` function imputes missing values using a user-defined statistical method (e.g. mean, median, max). Its default is *median*. For more advanced approaches, users could apply the `aregImpute()` function (from the R package **Hmisc**) which provides mean imputation using additive regression, bootstrapping, and predictive mean matching.

We also can remove NA values from the *adult* dataset by using `na.omit()`. Note, we normally should not remove the missing values. It depends on the situation whether it's a good idea or not. You shouldn't always just drop NA values.

## Outliers for variable `capital.loss`

Here we want to check if there are any outliers in the variable `capital.loss` or not. For this, first, we report the summary of the variable `capital.loss` as follow:

```{r}
summary( adult $ capital.loss )
```

We also report the boxplot and histogram of the variable `capital.loss` as follow:

```{r fig.align = 'center'}
ggplot( adult ) +
     geom_boxplot( aes( y = capital.loss ) )
```

```{r fig.align = 'center'}
ggplot( adult ) +
     geom_histogram( aes( x = capital.loss ), bins = 30, color = "blue", fill = "lightblue" )
```

**What would be your interpretation of the above outputs for the variable `capital.loss`? Are there any outliers in the variable `capital.loss`? If so, what would be a good strategy to deal with the outliers?**

<details class="answer_panel">
  <summary>Answer</summary>

**Interpretation of 4.4 outputs**

The summary output for `capital.loss` shows that the data is positively skewed, because the median < mean. Furthermore, we can report the extreme values from the summary; the smallest value that `capital.loss` can take on is \$0, while the largest one is \$4365. Lastly, we can conclude from the summary output that more than 75% of the data consists of \$0 observations, based on the first, second (median) and third quartiles. Furthermore, on average the individual of the data set has suffered from a capital loss of $87.94.

The second plot is a boxplot for `capital.loss` and it shows an immense positive skewness towarsd $0. Not only the distribution of the data can be read from the plot but also the extreme values reported above; minimum of \$0 and maximum of approximately \$4300.

The last graph is a histogram, that collects observation frequencies into bins of width 30. We see a large quantity of \$0 observations, and some other observations in the middle of the graph around 2000. Although we cannot see the maximum value from the graph, we can just suspect that it is somewhere around 4000, since this is how far the x-axis of the plot stretches. This alludes to fact that the right extreme is around \$4000.

Given that the above histogram's y-axis is adjusted to the large amount of \$0 capital losses, it does not allow us to see the distribution of non-zero observations. A remedy for that is to adjust the limit of the y-axis to [0, 1000], and plot the new graph.

```{r fig.align = 'center'}
ggplot( adult ) +
  geom_histogram( aes( x = capital.loss ), bins = 30, color = "blue", fill = "lightblue" ) +
  coord_cartesian( ylim = c( 0, 1000 ) )
```

There are two peculiarities that immediately standout from the graph. First, the large count of \$0 capital losses. Second, the approximately symmetric distribution of the non-zero losses.

Below we interpret the two peculiarities separately.

**Large count of 0 observations**
 
The dataset contains `r length(which(adult $ capital.loss == 0))` obeservations that are \$0 for `capital.loss`. Per definition, capital loss, is a loss incurred when an investment is sold for less than the price it was originally purchased for. Unlike in the case of the `diamonds` datatset whereby 0mm for `x`, `y` or `z` was an invalid record, 0 for `capital.loss` is a valid observations of which root cause may be multiple fold.

1. The individual made an investment, but did not sell the asset during the time window the data was recorded, and hence the investor had not suffer any realised losses, i.e., capital loss.
2. The individual made an investment, and sold it at the exact same price the asset was originally purchased for, and hence incurred 0 capital loss as well as gain.
3. The individual made an investment, did not incur capital loss, but capital gain.
4. The individual did not make an investment and hence could not incur any capital losses, nor gains on the investment.

Considering the validity of each of the above points, for the first point the data documentation does not make any indication that the dataset records unrealised losses as \$0 capital losses. Therefore, we can rule out that the large count of \$0 observation is due to unrealised saldos.

The validity of point number 2 also lies on shaky grounds. Firstly, markets tend to have some level of movements that has a direct impact over investors capital balance. Hence, it is highly unlikely that one manages a portfolio in a way that it ends up being in exactly null-saldo, i.e., in \$0 capital gain and loss. Second, individuals invest with the primary purpose of making profits, instead of closing off the investment at \$0 gain and loss. Therefore, we can conclude that the large count of \$0 observations are unlikely because of the investors sold the investment at the same price it was purchased for.

Consequently, point number 1 and 2 are week enough assumption to be relied on, and so discarded.

Considering point number 3, we must test both `capital.loss` and `capital.gain` as shown below.
```{r}
# is there any observations, where both capital.gain
# and capital.loss is greater than zero
any(ifelse(adult $ capital.loss != 0 & adult $ capital.gain != 0, TRUE, FALSE))
any(ifelse(adult $ capital.loss == 0 & adult $ capital.gain != 0, TRUE, FALSE))
any(ifelse(adult $ capital.loss != 0 & adult $ capital.gain == 0, TRUE, FALSE))
```

Given the above outputs, we can conclude, that `capital.loss` and `capital.gain` are mutually exclusive values. That is, if one makes an investment, the investor either makes a loss or gain, but not the two together. This realisation leads us to point number 4, which questions whether the individual made an investment at all. This must be the case when both attributes, `capital.loss` and `capital.gain` are that of \$0. 

All things considered above, the large amount of zero observations in `capital.loss` boils down 2 factors. First, the person made a gain on the investment, and hence did not incur any losses. Second, the individual did not make any investment and hence could not incur either losses or gains.

**Approximate normality of the non-zero observations**

For the non-zero observations, that is a subset of people who incurred capital loss, we see an approximate bell curved distribution. However, currently, `capital.loss` is burdened with the large quantity of \$0 observations, therefore, it skews the data positively. To eliminate the skewness and to be able to draw meaningful statistics we replace \$0 observations with NA and report the mutated data.

```{r}
adult_2 = mutate( adult, capital.loss = ifelse( capital.loss == 0, NA, capital.loss ) )

summary(adult_2 $ capital.loss)
```

We report that 46316 observations were replaced by NA, that is there are 46316 individuals who either did not invest or gained on their investments. Furthermore, we can report that the data is approximately bell-curve distributed and centered around mean losses of \$1873. The largest subset of losers is represented by the middle 50% of the distribution. It consists of individuals who reported capital losses between \$1672 and \$1977. The left tail of the histogram represents 25% of the distribution who lost between \$155 and \$1672. The right tail represents another 25% of the non-zero observations, investors who lost from \$1977 up to \$4356. The shape of the curve is reasonable, since it is usual to have a wide range of investors losing an average amount, while only a small fraction of those who lose either a small or rather considerable amount.

**Outliers in `capital.loss`**

To identify the outliers for `capital.loss` we created a boxplot with the cleaned up data.

```{r}
boxplot(adult_2 $ capital.loss, horizontal = TRUE)

quantile(adult_2 $ capital.loss, na.rm = TRUE)

q1 = unname(quantile(adult_2 $ capital.loss, 0.25,  na.rm = TRUE))
q3 = unname(quantile(adult_2 $ capital.loss, 0.75,  na.rm = TRUE))
iqr = q3 - q1
whisker_left = q1 - 1.5 * iqr
whisker_right = q3 + 1.5 * iqr
```

The plot and the above calculation shows that any values below the left whisker, `r whisker_left`, and above the right whisker, `r whisker_right` are deemed outliers. This regards on the left `r length(which(adult_2 $ capital.loss < whisker_left))`, and on the right `r length(which(adult_2 $ capital.loss > whisker_right))` of observations.

**Strategy to deal with the outliers**

Since we assumed for the shape of the distribution an approximate bell-curve, we would replace the outliers with values taken from a random normal distribution centered around the same mean as the original data. This way we can guarantee the measure of location and the spread to preserve its original shape.

```{r}
adult_3 = mutate(adult_2, capital.loss = ifelse(capital.loss < whisker_left | capital.loss > whisker_right, rnorm(1, mean = 1873), capital.loss))

ggplot( adult_3 ) +
  geom_histogram( aes( x = capital.loss ), bins = 30, color = "blue", fill = "lightblue" )

summary(adult_3 $ capital.loss)
```

</details>

## Outliers for variable `capital.gain`

Here we want to check if there is any outliers in the variable `capital.gain` or not. 

a. **Similar to the previous part report the `summary`, `boxplot`, and `histogram` for the variable `capital.gain`.**

<details class="answer_panel">
  <summary>Answer a</summary>
  
```{r}
summary(adult $ capital.gain)

ggplot( adult ) +
     geom_boxplot( aes( y = capital.gain ) )

ggplot( adult ) +
     geom_histogram( aes( x = capital.gain ), bins = 30, color = "blue", fill = "lightblue" )
```


</details>

b. **What would be your interpretation of the above outputs for the variable `capital.gain`? Are there any outliers in the variable `capital.loss`? If so, what would be a good strategy to deal with the outliers?**

<details class="answer_panel">
  <summary>Answer b</summary>

**Interpretation of output 4.5/a**

The first output of 4.5/a is a descriptive statistic of `capital.gain`. By reading the "Min." and "Max." values of the summary for `capital.gain` we can conclude that the data is distributed over the range of [0, 41310]. The first, second (median) and thrid quartiles refer to the 25th, 50th and 75th percentile of the data. They are all \$0, meaning that at least 75% of the data consists of \$0 observations for `capital.gain`. The mean number of 582.4, refers to the mean value an individual realises as capital gain in the dataset in dollar terms. From the above summary, we can conclude that the data skewed positively since the median < mean. 

The second output is a boxplot demonstrating the locality, spread and the skewness of `capital.gain`. Again, we read a large quantity of 0 observation, and some other values saturating below 10000, that thins up above 10000. We can approximate the extreme values of the data by reading the smallest and largest observation of the boxplot. Minimum being at 0, and the maximum a little bit above 40000. The positively skewed nature of the distribution is clear from the boxplot.

The last output is a histogram for `capital.gains`' value frequency compiled into bins of size 30. The 0 observations for `capital.gain` is significant from the plot — it almost reaches 45000 observations.

**Outliers in `capital.gain`**

We see from the above summary output, that the first and the third quartile are at \$0, meaning that at least 75% of `capital.gain` is 0. Similarly to exercise 4.4, we replace the 0 observations with NA values, so to limit down the observations to individuals who invested and realised capital gains. As shown below, we reported the summary of the mutated data along with a boxplot.  

```{r}
adult_4 = mutate( adult_3, capital.gain = ifelse( capital.gain == 0, NA, capital.gain ) )

summary(adult_4 $ capital.gain)
```

We see from the above summary that the 44807 zero observations were replaced by NA values.

To have an idea of the mutated data's distribution we replotted a histogram.

```{r}
ggplot( adult_4 ) +
     geom_histogram( aes( x = capital.gain ), bins = 30, color = "blue", fill = "lightblue" )
```

The positive skewness is still present in the distribution, nevertheless, we report a boxplot of it to see potential outliers.

```{r}
boxplot(adult_4 $ capital.gain, horizontal = TRUE)
q1 = unname(quantile(adult_4 $ capital.gain, 0.25,  na.rm = TRUE))
q3 = unname(quantile(adult_4 $ capital.gain, 0.75,  na.rm = TRUE))
iqr = q3 - q1
whisker_left = q1 - 1.5 * iqr
whisker_right = q3 + 1.5 * iqr
```

Based on the above calculations, values beyond `r whisker_right`, are so distant from the majority of the data, that they are outliers. This regards `r length(which(adult_4 $ capital.gain > whisker_right))` observations.


**Strategy to deal with the outliers**

The distribution of `capital.gain` does not show strong enough similarities to normality or symmetry, therefore we do not apply the same imputation method as in exercise 4.4. Since `capital.gain` is a numerical attribute, we replace the above qualified outliers with the mean value of the data, and replot it.

```{r}
adult_5 = mutate(adult_4, capital.gain = ifelse(capital.gain > whisker_right, mean(capital.gain), capital.gain))

ggplot( adult_5 ) +
  geom_histogram( aes( x = capital.gain ), bins = 30, color = "blue", fill = "lightblue" )
```

Finally, to preserve the eliminated 0 observations for both `capital.gain` and `capital.loss` we add a flag variable for future reference, to be able to make a distinction between individuals who invested and those who did not.

```{r}
adult_5["has.invested"] = ifelse(adult["capital.gain" == 0] & adult["capital.loss" == 0], FALSE, TRUE)
```

</details>


# Business Understaning Stage for your main project

For data-driven business-making, it is vital to understand the problem to be solved. This may seem obvious, but business projects seldom come pre-packaged as clear and unambiguous data mining problems. 

The Business Understanding stage represents a key part of the craft where the analysts' creativity plays a large role. Data Science has some things to say, as we will describe, but often the key to a great success is a creative problem formulation by some analyst regarding how to cast the business problem as one or more data science problems.  High-level knowledge of the fundamentals helps creative business analysts see novel formulations. The Business Understanding stage can be done by following steps:

1. First, clearly enunciate the project objectives and requirements in terms of the business or research unit as a whole.
2. Then, translate these goals and restrictions into the formulation of a problem that can be solved using data science.
3. Finally, prepare a preliminary strategy for achieving these objectives.

**Your task** for this part is to write a Business problem that you have (or you had) in your company. Write the problem done by following the above three steps for the Business Understanding stage and discuss it with your teammate. Each group should represent the result in the class for around 5 minutes. 

Do you think the Business/Research problem that you have is interesting and can be solved using data science? If so, provide your reasons. Just remember that you could consider it as the main project of this course. It means it can cover 22 percent of your final grade. 

If you have an interesting Business/Research problem, I highly recommend you define it as the main project of your course with your teammates. I think in this way this course for you would be more effective and I will support you.



