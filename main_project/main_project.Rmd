---
title: '<center> How to increase employee retention in the field of Data Science <center>'
author: '<center> Peter Molnar <center>'
date: '<center> `r Sys.Date()` <center>'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 5
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk $ set( echo = TRUE, message = FALSE, warning = FALSE, error = FALSE, fig.align = 'center'  )
```

```{r}
# dependencies
library(readr)            # For the read_csv function
library( naniar  )        # for missing value plotting
library( Hmisc  )         # for imputation
library( dplyr  )         # for select function
library( ggplot2 )        # For visualization
library( GGally )         # For correlation plots
library( psych )          # For correlation plots  
library( liver )          # For partition function
library( rpart )          # For the "CART" algorithm
library( rpart.plot )     # To plot decision trees
library( C50 )            # For the "C5.0" algorithm
library( randomForest )   # For the "Random Forest" algorithm
library( pROC )           # For ROC plot
```

```{r results = FALSE, echo = FALSE }
# To ensure consistent formatting of the plots, we are using a colour theme:
theme_new = theme( panel.background = element_rect( fill = "white", colour = "white", size = 0.5, linetype = "solid" ),
                   panel.grid.major = element_line( size = 0.2, linetype = 'solid', colour = "gray77" ), 
                   panel.grid.minor = element_line( size = 0.1, linetype = 'solid', colour = "gray90" ),
                   axis.text  = element_text( size = 11 ), 
                   axis.title = element_text( size = 12, face = "bold" ),
                   title = element_text( size = 14, face = "bold" )
                  )
```

# Business Understanding Stage

## General understanding of the problem

Employee turnover refers to the event of replacing current employees with new ones. Employee acquisition is a long and costly procedure; a given position needs to be advertised, multiple candidates be selected and have to go through multiple rounds of interviews. Hence turnover is one of the most persistent and frustrating problems that organisations have long been facing.

Whether it is involuntary, such as termination due to poor performance, or voluntary, such as resignations, turnover is extremely costly.

According to a conservative estimates by the Bureau of Labor Statistics, on average an employee replacement costs approximately \$13,996 per employee. However, this amount for the Information Technology (IT) sector on average reaches \$20,000 per employee. As the economy grows, more jobs, and therefore opportunities are created, and therefore voluntary turnover also increases. 

Organisations tend to underestimate the true cost of turnover, and hence does not focus on employee retention probperly. This may be because, turnover is not an accountable line-item in most profit and loss statements, nor is it typically defined in the budget, not to mention that no one submits an invoice at the end of the month for turnover.

In addition to the explicit replacement fees, there are implicit costs as well, such as productivity loss, workplace safety issues, and morale damage to the company, that further deepens the severety of employee turnover.

Since there is not much organisations can do to dramatically reduce the costs associated with turnover, the best measure is to reduce turnover directly by an improved selection process that assesses candidates' turnover risk, motivational fit early in the hiring process. Such hiring process helps to reduce turnover, and can lead to a safer, more productive and profitable work place.

## Dataset description

The selected dataset, ["HR Analytics: Job Change of Data Scientists"](https://www.kaggle.com/datasets/arashnic/hr-analytics-job-change-of-data-scientists), is on a Big Data and Data Science specialised company, that seeks to hire candidates. The company implemented a series of training courses, that anyone could sign-up, and upon completion the candidate is offered a position. The dataset contains information about the candidates’ demographics, education, experience that are collected during the sign-up process for the training. By that, the company aims to narrow down candidates to only those who are really interested to work for the company, and identify those who are likely to opt out after the training was given.

This paper aims to identify factors associated with employee turnover, and employees who are likely to be open for new opportunities after the training given.

* What characteristics do candidates bear with that the company loses?
* What are the key factors of employee turnover?
* What to do differently to retain talents?

# Data Understanding Stage

Below, we import the dataset into the environment by using the `read_csv` function.
```{r}
data = read_csv("hr_dataset.csv")
``` 

Next we report some descriptive statistics and the structure of the dataset.
```{r}
summary(data) # descriptive statistics
str(data)     # structure of the data
```

The above outputs shows, that data is imported as a data.frame object, and that it consists of `r nrow(data)` rows and `r ncol(data)` columns/variables. The `r ncol(data)` variables includes the target variable, called identified by the variable called `target`.

The [official dataset](https://www.kaggle.com/datasets/arashnic/hr-analytics-job-change-of-data-scientists) delivers some explanation to the features:

| Feature | Feature description |
| :- | :- |
| `enrollee_id` |  Unique ID for candidate |
| `city` |  City code |
| `city_development_index` |  Development index of the city (scaled) |
| `gender` |  Gender of candidate |
| `relevent_experience` |  Relevant experience of candidate |
| `enrolled_university` |  Type of University course enrolled if any |
| `education_level` |  Education level of candidate |
| `major_discipline` |  Education major discipline of candidate |
| `experience` |  Candidate total experience in years |
| `company_size` |  No of employees in current employer's company |
| `company_type` |  Type of current employer |
| `lastnewjob` |  Difference in years between previous job and current job |
| `training_hours` |  training hours completed |
| `target` |  0 – Not looking for job change, 1 – Looking for a job change |

Although, the official summary on the variables gives some understanding about what the different variables stand for, in the coming section we further delve into them.

## Understanding the variables

The above structure output reports that the following variables are imported as character variables:
`city`, `gender`, `relevent_experience`, `enrolled_university`, `education_level`, `major_discipline`, `experience`, `company_size`, `company_type`, `last_new_job`, and the followings as numerical variables: `city_development_index`, `enrollee_id`, `target`, `training_hours`.

In this section, first we aim to gain understanding on the potential unique values each of the character variables may take on. By knowing this, in a later section ([data cleaning and preprocessing](#section_cleaning_preprocessing)) can better clean up and convert the data to the corresponding data type.

```{r}
categorical_vars = names(Filter(is.character, data)) # filter out column names with type character
categorical_data = data[,c(categorical_vars)] # select columns of type character

lapply(categorical_data, unique) # iterate through the selected columns, and report the unique values
```

### `city` {.unlisted .unnumbered}

The `city` variable holds `r length(unique(categorical_data $ city))` unique values, which are identifiers of cities of the candidate's current employer. Each value follows the same naming convention; `city_<id>`, whereby id stands for a unique numerical identifier of the city. In such format, the `city` variable is anonymised, and since the dataset does not provide any information on the geographical location of the city, we cannot attach existing geographical knowledge to it.


### `gender` {.unlisted .unnumbered}

The `gender` variable holds `r length(unique(categorical_data $ gender))` unique values, and stand for the gender of the candidate. The variable takes on the following unique values: `r unique(categorical_data $ gender)`.

### `relevent_experience` {.unlisted .unnumbered}

The `relevent_experience` variable holds `r length(unique(categorical_data $ relevent_experience))` unique values, and stands for whether the candidate has any relevant experience for the job or not. The variable takes on the following unique values: `r unique(categorical_data $ relevent_experience)`.

Based on the above unique values, we classify `relevent_experience` as a binary variable; that is, a candidate can either have relevant experience or not. Therefore, in a later section, this variable will be converted into a factor of 2 levels.

Furthermore, it is worth to remark that both the variable name and the values contain a spelling mistake - this will be taken care of in the [data cleaning and preprocessing](#section_cleaning_preprocessing) section.


### `enrolled_university` {.unlisted .unnumbered}

The `enrolled_university` variable holds `r length(unique(categorical_data $ enrolled_university))` unique values, and stands for type of the university program the candidate was enrolled to, if any.

The variable can take on the following unique values: `r unique(categorical_data $ enrolled_university)`.

As a remark, the observations "no_enrollment" is not consistent with the rest of the values' naming convention. It is delimited with underscore and non-capitalised. Therefore, this inconsistency will be taken care of in the [data cleaning and preprocessing](#section_cleaning_preprocessing) section.


### `major_discipline` {.unlisted .unnumbered}

The `major_discipline` variable holds `r length(unique(categorical_data $ major_discipline))` unique values, and stands for the candidate's major studies, if any.

The variable can take on the following unique values: `r unique(categorical_data $ major_discipline)`.


### `experience` {.unlisted .unnumbered}

The `experience` variable holds `r length(unique(categorical_data $ experience))` unique values, and stands for the number of years of experience the candidate has, if any.

The variable can take on the following unique values: `r unique(categorical_data $ experience)`. As a clarification, the observation "<1" means less than a year of work experience, and ">20", more than 20 years in duty.


### `company_size` {.unlisted .unnumbered}

The `company_size` variable holds `r length(unique(categorical_data $ company_size))` unique values, and stands for the headcount of the company the candidate is currently working for, if any.

The variable can take on the following unique values: `r unique(categorical_data $ company_size)`. The values such as "<10", stands for a company with less than 10 employees, and "100-500" for an organisation with headcount between 100 and 500 employees.

It is worth to note, that the values of this variable is inconsistent and deviates from the standard convention of the rest of the variables. This problem will be addressed in the [data cleaning and preprocessing](#section_cleaning_preprocessing) section.

### `company_type` {.unlisted .unnumbered}

The `company_type` variable holds `r length(unique(categorical_data $ company_type))` unique values, and stand for the type of the company the candidate currently is working for, if any.

The variable can take on the following unique values: `r unique(categorical_data $ company_type)`.


### `last_new_job` {.unlisted .unnumbered}

The `last_new_job` variable holds `r length(unique(categorical_data $ last_new_job))` unique values, and stands for the difference in years between the previous job and current job.

The variable can take on the following unique values: `r unique(categorical_data $ last_new_job)`. Whereby "never" refers to no previous job, and ">4" to the last job being later than 4 years.

Here as well a remark to be made, that the value "never" does not necessarily align with the other numerical values, and therefore, this will be taken care of in the [data cleaning and preprocessing](#section_cleaning_preprocessing) section.



# Data cleaning and preprocessing {#section_cleaning_preprocessing}

## `enrollee_id` {.unlisted .unnumbered}
Since `enrollee_id` is a unique identifier of each of the candidates that signed-up for the training, it carries no valuable information on the candidate, and therefore it is redundant in the dataset, hence it is removed.

```{r}
col_idx = match( "enrollee_id", names(data) ) # get column index position of 'enrollee_id'
data = data[, -col_idx]                       # remove enrollee_id column from dataset
```


## `relevent_experience` {.unlisted .unnumbered}

Per mentioned above, th `relevent_experience` contains a spelling mistake and hence is replaced with "relevant_experience".
```{r}
names(data)[names(data) == 'relevent_experience'] = "relevant_experience"
```

Furthermore, for such a binary variable we aim to use a clear notation, to indicate whether the candidate has relevant experience or not. We achieve this by replacing "Has relevent experience" with "Yes" and "No relevent experience" with "No" values, so to make later interpretations easier.

```{r}
data $ relevant_experience[data $ relevant_experience == "Has relevent experience"] = "Yes"
data $ relevant_experience[data $ relevant_experience == "No relevent experience"] = "No"
```


## `enrolled_university` {.unlisted .unnumbered}

In case of `enrolled_university`, the column values “no_enrollment”, not only contains a spelling mistake, but also inconsistent with the other 2 values for this variable. I.e., it is delimited with underscore and non-capitalised. The below code replaces “no_enrollment” observations with “No enrolment” 
```{r}
data $ enrolled_university[data $ enrolled_university == "no_enrollment"] = "No enrolment"
```


## `company_size` {.unlisted .unnumbered}

The `company_size` variable contains inconsistent and overlapping observations. The value "10/49", should represent an interval, instead of a quotient, and therefore it should be delimited with a dash, instead of forward slash.

Below we replace each "10/49" values with "10-49".
```{r}
data $ company_size[data $ company_size == "10/49"] = "10-49"
```

Furthermore, the intervals of `company_size` should be mutually exclusive values. However, in the case of the following 2 values, "100-500" and "500-999", the "500" overlaps between the two categories. So, next we replace the "100-500" observations with "100-499". Additionally, the value "10000+" does not follow the convention either, so it is replaced with ">9999".

```{r}
data $ company_size[data $ company_size == "100-500"] = "100-499"
data $ company_size[data $ company_size == "10000+"] = ">9999"
```


## `last_new_job` {.unlisted .unnumbered}

The `last_new_job` column contains the observation "never", that expresses that the candidate never had a job. For the sake of following the convention, and keep the data as numeric as possible, this value is replaced with "0".

```{r}
data $ last_new_job[data $ last_new_job == "never"] = "0"
```

## `target` {.unlisted .unnumbered}

Lastly, the target variable, `target`, is not explanatory for the dataset. Therefore, we change the column name to something more descriptive, e.g., `is_looking_for_job`, representing binary observations, whether the individual is looking to change job or not.

```{r}
names(data)[names(data) == 'target'] = "is_looking_for_job"
```

Furthermore, we replace the current numerical observations with character ones. More specifically, the '1' with "yes" and the '0' with "no".

```{r}
data $ is_looking_for_job[data $ is_looking_for_job == 1] = "yes"
data $ is_looking_for_job[data $ is_looking_for_job == 0] = "No"
```


## Dealing with missing values

The below code reports the percentage of missing values for each variable.

```{r}
(miss_var_summary( data ))
```

We see from the above output, that `company_type`, `company_size`, `gender`, `major_discipline` suffer from a large quantity of missing values. Given the vast amount of missing values, we cannot perform any reliable imputation methods, therefore, we perform a more conservative approach, and introduce a new value, "unknown".

```{r}
severe_missing_val_cols = c("company_size", "company_type", "gender", "major_discipline")

for (col_name in severe_missing_val_cols)
{
  data[is.na(data[col_name]), col_name] = "Unknown"
}
```

The other variables that also suffer from missing values, are  `education_level`, `last_new_job`, `enrolled_univeristy`, `experience` we impute them with values which is proportional to categories’ records.

```{r}
data $ education_level = impute(data $ education_level, "random")
data $ last_new_job = impute(data $ last_new_job, "random")
data $ enrolled_university = impute(data $ enrolled_university, "random")
data $ experience = impute(data $ experience, "random")
```

Next, we report a summary on the imputed values.
```{r}
imputed_vars = c("education_level", "last_new_job", "enrolled_university", "experience")

for (var in imputed_vars)
{
  print(var)
  summary(data[var])
}
```

Finally, we confirm if there is still any missing values left in the dataset.
```{r}
any( is.na(data) )
```

The above output shows, that there is no missing left in the dataset, so as a next step in the data cleaning and preprocessing, we move on with converting the variables to their appropriate data type.

## Converting the variables

Below the table summarises the data types of the variables.

| Statistical type of variable | variable |
| :- | :- | 
| numerical continuous | `city_development_index` |
| numerical discrete | `training_hours` |
| ordinal | `education_level`, `experience`, `company_size`, `last_new_job`|
| binary  | `relevant_experience`, `is_looking_for_job` |
| nominal  | `city`, `gender`, `enrolled_university`, `major_discipline`, `company_type` |

```{r}
# convert ordinal variables to ordered factor
educ_ordered_levels = c("Primary School", "High School", "Graduate", "Masters", "Phd")
data $ education_level = factor(data $ education_level, ordered = TRUE, levels = educ_ordered_levels)

experience_ordered_levels = c("<1", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", ">20")
data $ experience = factor(data $ experience, ordered = TRUE, levels = experience_ordered_levels)

comp_size_ordered_levels = c("Unknown", "<10", "10-49", "50-99", "100-499", "500-999", "1000-4999", "5000-9999", ">9999")
data $ company_size = factor(data $ company_size, ordered = TRUE, levels = comp_size_ordered_levels)

last_job_ordered_levels = c("0", "1", "2", "3", "4", ">4")
data $ last_new_job = factor(data $ last_new_job, ordered = TRUE, levels = last_job_ordered_levels)

# convert variables to factor
data $ relevant_experience = as.factor(data $ relevant_experience)
data $ is_looking_for_job = as.factor(data $ is_looking_for_job)

# convert nominal variables to factor
data $ city = as.factor(data $ city)
data $ gender = as.factor(data $ gender)
data $ enrolled_university = as.factor(data $ enrolled_university)
data $ major_discipline = as.factor(data $ major_discipline)
data $ company_type = as.factor(data $ company_type)
```

Now that the data is cleaned up and the variables are in the right type, we report the structure and summary again.

```{r}
summary(data)
str(data)
```
Next, we move on with exploring the data by means of Exploratory Data Analysis.

# Exploratory Data Analysis {#eda}

This section aims to identify useful predictors of `is_looking_for_job` target variable. We use graphs, plots, and tables to uncover important relationships that could indicate important areas for further investigation. Here we start with the categorical variables then numerical variables.

## Investigate the target variable `is_looking_for_job` {.unlisted .unnumbered}

First we investigate the target variable, and report a bar plot and summary.

```{r fig.height = 5, fig.width = 5}
ggplot( data = data, aes( x = is_looking_for_job, label = scales::percent( prop.table( stat( count ) ) ) ) ) +
  labs( title = "Bar plot for the target variable" ) +
  geom_bar( fill = c( "palevioletred1", "darkseagreen1" ) ) + 
  geom_text( stat = 'count', vjust = 0.2, size = 6 ) + 
  theme_new

summary( data $ is_looking_for_job )
```


$P(is\_looking\_for\_job = no) = 14381$

$P(is\_looking\_for\_job = yes) = 4777$

$n = P(is\_looking\_for\_job = no) + P(is\_looking\_for\_job = yes) = 14381 + 4777 = 19158$

$P(is\_looking\_for\_job = yes) = \frac{P(is\_looking\_for\_job = yes)}{n} = \frac{4777}{19158} = 24.93\%$

We see above, that the majority of the candidates, after successful completion of the training want to stay with the company. Still, `r round( summary( data $ is_looking_for_job )[2] / nrow( data ), 4 ) * 100`% of the candidates look for new opportunities. For the first sight the `r round( summary( data $ is_looking_for_job )[2] / nrow( data ), 4 ) * 100`% retention rate looks good, however, to understand the problem, we need to look at the other side of the same coin and see that candidate turnover rate is `r round( summary( data $ is_looking_for_job )[2] / nrow( data ), 4 ) * 100`%. It means, that on average, every fourth candidate wants to leave the company after the training, and technically uses the training courses given as a jumping board, and exploits the opportunity at the cost of the company.

Throughout the analysis below, we attempt unveil important relationships between the target variable, `is_looking_for_job`, and the rest of the variable. Furthermore, we aim to understand what are candidates that the company would need to target in order to reduce the turnover rate.


## Investigate the `city` variable {.unlisted .unnumbered}

With the `city` variable, we aim to understand which are the cities where the most candidates came from and the ones that were most successful in terms employment. Therefore, we plot the variable as an unstandardised and a standardised histogram.

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
ggplot( data = data ) + 
  geom_bar( aes( x = city, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for the target variable 'is_looking_for_job'" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = city, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new
```

Because the x-axis does not allow for clear identification of the cities, we make a new data frame the `city` data and the target variable. Below we add a few new columns to the data frame. These are, candidate count, which is the sum of the candidates from a given city. We also add application rate, which is rate of application per city compared to the overall dataset. Finally, we add a turnover rate column, that shows the percentage of candidates that wants to change job in the given city.

```{r}
city_df = as.data.frame.matrix((table(data $ city, data $ is_looking_for_job)))

names(city_df) = c("looking for job", "not looking for job")
city_df["candidate count"] = city_df $`looking for job` + city_df $`not looking for job`
city_df["application rate"] = round(city_df $`candidate count` / length(data $city), 4)
city_df["turnover rate"] = round(city_df $`looking for job` / city_df $`candidate count`, 4)
```

First we order the data in a decreasing order based on application rate, so that we see from which city the company gets the most applicants from. Below we report the top 5 cities based on application rate. We see that from the below 5 cities, more than 55% of the applications came.

```{r}
head(city_df[ order(city_df $`application rate`, decreasing = TRUE), ])
```
Next, we want to filter for the turnover rate, and order it increasing order, so that we retrieve cities with the lowest turnover rate first. However, first we filter for those cities where the candidate count were more than 30, so that we can eliminate those cities that were lacking any applications and would distort the interpretation.

```{r}
cand_count_more_30 = city_df[city_df $`candidate count` > 20, ]
head(cand_count_more_30[ order(cand_count_more_30 $ `turnover rate`, decreasing = FALSE), ])
```
We see that in the above cities, turnover rate is significantly higher than for the dataset as a whole.

Next we test whether there is significant correlation between the application rate, and the turnover rate of the given city.

**Hypotheses:** 

\[
\bigg\{
\begin{matrix}
          H_0:  \rho_{application_rate, turnover_rate} \leq 0 \\
          H_a:  \rho_{application_rate, turnover_rate} \neq 0
\end{matrix}
\]

**Test results:**
```{r}
cor.test( x = city_df $`application rate`, y = city_df $`turnover rate`, 
          alternative = "two.sided")
```

**Test conclusion:**

Given significance level of $\alpha = 0.05$ , and p-value = $85.68\%$ > $\alpha = 5\%$, there is insufficient evidence found to reject the $H_0$ in favour for $H_a$, and we infer that there is no linear relationship between application rate and turnover.

Therefore we can say, that just because there are a few cities that bears with a larger candidate count, it will not tell whether they will want to stay to work for a company or not. Overall, we can conclude that there is no predictive power of `city`, and hence we do not expect this variables to be part of the prediction model.

## Investigate the `gender` variable {.unlisted .unnumbered}

To understand whether gender is a predictor of whether the candidate is going to stay or not, we first plot it as a regular bar plot, with `is_looking_for_job` overlay, and then standardise it, so that we can read the the percentage rate of those who would like to stay with the company and those who do not.

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
addmargins( table( data $ is_looking_for_job, data $ gender, dnn = c( "is_looking_for_job", "gender" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = gender, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'gender', with 'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = gender, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme_new
```

The above contingency table quantifies the relationship between the different gender categories and turnover rate. Namely, we see that the gender ratio is dominated by male, 13221 observations (approx. 69%). While this number seems overly large, we have to remind ourselves that the data is on Data Science positions. So it should not come as a surprise, that on an engineering field is mostly male dominated. The female gender represents themselves by 1238 candidates (approx. 6.5%), and those of Unknown and Other gender types take up the rest 24.5%.

From the standardised plot we see that the Unknown category produces the highest turnover among the genders, while the different genders (Female, Male, Other) are about on the same level as the turnover rate of the dataset. 

Since the above plot does not provide strong graphical evidence of `gender` being an important predictor, we test the proportions of genders by a hypothesis test.

**Hypotheses:** 

\[
\left\{ \begin{array}{l}
         \mbox{$H_0:  \pi_{Femal, yes}  = \pi_{Male, yes} = \pi_{Other, yes} = \pi_{Unknown, yes}$} \\
         \mbox{$H_a:$ at least one of the claims under $H_0$ is wrong} \end{array} \right.
\] 

**Test results:**
```{r}
chisq.test( table( data $ is_looking_for_job, data $ gender ) )
```

**Test conclusion:**

Given significance level of $\alpha = 0.05$ , and p-value = $2.2*10^{-14}\%$ < $\alpha = 5\%$, there is sufficient evidence found to reject the $H_0$ in favour for $H_a$, and we infer that there is statistical relationship between `gender` and the target variable.

Now that the relationship is confirmed, the company needs to give attention to those candidates with Unknown gender category, because they tend to leave the company more than the rest of the genders.


## Investigate the `relevant_experience` variable {.unlisted .unnumbered}

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
addmargins( table( data $ is_looking_for_job, data $ relevant_experience, dnn = c( "is_looking_for_job", "relevant_experience" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = relevant_experience, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'relevant_experience', with 'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = relevant_experience, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme_new
```

The above contingency table quantifies the relationship between `relevant_experience` and the target variable. We read that 13792 (71.9%) candidates have experience, and the rest 5366 (28.1%) do not. It means that approximately, the dataset contains approximately 2.5 times more of those candidates who have experience than those who do not.

The above plots, reinforces what we concluded from the contingency table. However, to learn about the tendency of looking for new opportunities across the different experience categories, we need to look at the plot with `is_looking_for_job` overlay. We see that the turnover rate is higher for those without experience (approximately 30%) and lower for those with experience (approximately 20%). Given the visible difference in turnover rate, we do not need to carry out any hypothesis test to confirm the predictive importance of `relevant_experience`.

Furthermore, this result also alludes to track carefully candidates based on their previous relevant experience, as those without tend to leave the company more compared to those without.


## Investigate the `enrolled_univeristy` variable {.unlisted .unnumbered}

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
addmargins( table( data $ is_looking_for_job, data $ enrolled_university, dnn = c( "is_looking_for_job", "enrolled_university" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = enrolled_university, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'enrolled_university', with 
        'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = enrolled_university, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme_new
```

The contingency table shows that the majority of candidates (14105) are/were not enrolled to any higher education, while those who took either full or part time courses at the univeristy are in minority (5053).

Next, we analyse the standardised plot and report that those with a full time degree tend to leave the company approximately 37.5% of the times, while those of the other categories approximately at a rate of 20-25%. 

Given the large and clear difference between the university enrolment categories, there is no need for a hypothesis test to confirm this. Therefore, based one the strong graphical evidence, we can expect the data mining algorithm to incorporate `enrolled_university` to the prediction model.

The above result also suggest, that the firm should address attention to candidates university level. That is, they should target those candidates who do not necessarily have a full time university degree.


## Investigate the `education_level` variable {.unlisted .unnumbered}

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
addmargins( table( data $ is_looking_for_job, data $ education_level, dnn = c( "is_looking_for_job", "education_level" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = education_level, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'education_level', with 
        'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = education_level, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new
```

The largest part of the dataset is represented by those with a graduate degree (bachelor). Meaning, they have the highest frequency in the dataset. The second and third largest representing group are those with Masters degree and High School diploma, respectively.

Given that `education_level` is an ordinal variable, we observe a rough normality, and symmetry in the data. That is, most of the candidates in the dataset hold a graduate degree, while towards the sides we find the rarer cases, that is those with Masters and High School qualifications. Towards the tails, we find the rarest observations, i.e., those with significantly lower and higher qualifications, such as Primary School diploma and Phd.

To see the proportion of turnover for each of the education levels, we read the standardised plot, and report that it kept its approximate symmetric shape. Furthermore, we see that those with a graduate degree tend to change job the most likely, by more than 25%, while the rest of the categories change with a likeliness of less than 25%. Those with High School degrees change jobs approximately with the same frequency as that of with Masters degree, which is approximately 20%. Similarly, those with Primary School degree wants to change job approximately as frequently as Phd degree holders, that is approximately 13%.

The above plots indicate strong graphical evidence of education level being an important predictor of the target variable, therefore, we can expect the data mining algorithm to incorporate it into the prediction model.

The above plot also highlights that the company needs to target those with Primary school certificate, or High school Diploma, or Masters degree or Phd. However, on the field of Data Science the higher qualification is generally more favoured, therefore the company should track candidates specifically with Masters and Phd degree.

## Investigate the `major_discipline` variable {.unlisted .unnumbered}

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
addmargins( table( data $ is_looking_for_job, data $ major_discipline, dnn = c( "is_looking_for_job", "major_discipline" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = major_discipline, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'major_discipline', with 
        'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = major_discipline, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new
```

From the above plots we read, that those with STEM (Science, technology, engineering, and mathematics) qualification rule the dataset (14492 observations). The second largest group that represents themselves in terms of major discipline is those with unknown major (2813 observations). The rest 1853 observation spread out between Art, Business, Humanities and No Major categories. 

The fact the STEM rules the data was expected, since Data Science is a STEM field, therefore it comes as no surprise, that we such a staggering number of STEM candidates.

What we are also interested in is the turnover rate between different major disciplines. From the standardised plot, we learn that there is hardly any difference between the categories and the turnover rate between them. To confirm our claim, we will test it by hypothesis test.

**Hypotheses:** 

\[
\left\{ \begin{array}{l}
         \mbox{$H_0:  \pi_{Arts, yes}  = \pi_{Business, yes} = \pi_{Humanities, yes} = \pi_{NoMajor, yes} = \pi_{Other, yes} = \pi_{STEM, yes} = \pi_{Unknown, yes}$} \\
         \mbox{$H_a:$ at least one of the claims under $H_0$ is wrong} \end{array} \right.
\] 

**Test results:**
```{r}
chisq.test( table( data $ is_looking_for_job, data $ major_discipline ) )
```

**Test conclusion:**

Given significance level of $\alpha = 0.05$ , and p-value = $6.225*10^{-10}\%$ < $\alpha = 5\%$, there is sufficient evidence found to reject the $H_0$ in favour for $H_a$, and we infer that there is statistical relationship between `major_discipline` and the target variable.

Therefore, we will include `major_discipline` into the prediction model.


## Investigate the `experience` variable {.unlisted .unnumbered}

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
addmargins( table( data $ is_looking_for_job, data $ experience, dnn = c( "is_looking_for_job", "experience" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = experience, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'experience', with 
        'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = experience, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new
```

The above plots show, a multi-modal distribution. That is a distribution with multiple local modes; the first one is around experience of 4 years, the second one around experience of 10 years and lastly at 15 years. Additionally, we report that those with more than 20 years of experience represent the majority of the dataset.

To see the turnover rate between the difference experience level, we read the standardised plot. The standardised plot shows that the more experience the candidate has the less likely he/she will be open to new job opportunities. Meaning, that there is a strong graphical indication of `experience` being an important predictor of the turnover rate, therefore we can expect this variable to be included in the prediction model by the data mining algorithm.

Furthermore, the above identified relationship suggests the company to target more senior candidates.


## Investigate the `company_size` variable {.unlisted .unnumbered}


```{r, fig.show = "hold", out.width = "50%", fig.align = 'default'}
addmargins( table( data $ is_looking_for_job, data $ company_size, dnn = c( "is_looking_for_job", "company_size" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = company_size, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'company_size', with 
        'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = company_size, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new
```

The above plot shows that most of the candidates with unknown company size represent the majority of the data. Next to that, those who work for companies of size "50-99" and "100-499" and large organisation of size more than ">9999".

To observe the turnover rate between the different categories, we read the standardised plot; those with unknown company size tend to look for a job in the largest percentage. Next to this observation, only one category stands out as being significantly different from others and this is the candidates from company size of "10-49". These candidates tend to look for a job approximately 23% of the cases. 

Conclusively, we see strong graphical indication of `company_size` as an important predictor of `is_looking_for_job`, and hence expect it to be incorporated in the prediction model.

The above data, and conclusion suggest the company not to take candidates with from companies of unknown size.

## Investigate the `company_type` variable {.unlisted .unnumbered}

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default'}
addmargins( table( data $ is_looking_for_job, data $ company_type, dnn = c( "is_looking_for_job", "company_type" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = company_type, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'company_type', with 
        'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = company_type, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new
```

The above plots show that the majority of the candidates work for private limited (9817 observations) and unknown (6140 observations) employers. Next to these two large categories, the rest observations (3201) spread roughly evenly across Early Stage Startups, Funded Startups, NGOs, Other companies, and the Public Sector.

To see the turnover rate per company type, we observe the standardised plot, and see that those with unkonwn company type tend to look for a new job in the largest percentage. The second largest type in terms of turnover rate is the Early Stage Startup. Therefore, we see strong graphical indication of `company_type` being a predictor of the target variable, hence we can expect `company_type` to be included in the model by the data mining algorithm.

Similarly to the previous variable, `company_size`, here the plots suggest the company not to hire candidates from unknown company type.


## Investigate the `last_new_job` variable {.unlisted .unnumbered}

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
addmargins( table( data $ is_looking_for_job, data $ last_new_job, dnn = c( "is_looking_for_job", "last_new_job" ) ) )

ggplot( data = data ) + 
  geom_bar( aes( x = last_new_job, fill = is_looking_for_job ) ) +
  labs( title = "Bar plot for 'last_new_job', with 
        'is_looking_for_job' overlay" )  +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new

ggplot( data = data ) + 
  geom_bar( aes( x = last_new_job, fill = is_looking_for_job ), position = "fill" ) +
  scale_fill_manual( values = c( "palevioletred1", "darkseagreen1" ) ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  theme_new
```

The above plots show that those who never had a job before, or changed job 1, 2 or more than 4 years ago are in majority. Those candidates who changed their job 3 or 4 years ago are in minority. The standardised graph clearly shows, that the longer a candidate stays at a specific workplace the less likely he/she will have the tendency to change. That is, the relationship between `last_new_job` and turnover rate is negative.

Given the strong graphical evidence of a relationship between `last_new_job` and the target variable, we expect the data mining algorithm to include the variable in question into the prediction model.

The above plot suggest the company to look for candidates who do not frequently change their job, instead stays as much as possible at the current place. 

## Detect Correlated Variables

From the EDA only the numerical variables left unexplored, `training_hours` and `city_development_index`.

First, we visualize the correlation matrix between: `training_hours` and `city_development_index` by using the `ggcorr()` function as follows:


```{r}
variable_list = c( "city_development_index", "training_hours" )
ggcorr( data = data[ , variable_list ], label = TRUE ) 
```

The correlation matrix shows no correlation between the two variables, therefore we can keep both variables for analysis.


## Investigate the `training_hours` variable {.unlisted .unnumbered}

Given that the `training_hours` variable is discrete numeric, we first plot the distribution, and then as a box plot so that we can see if the dataset contains any outliers.

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
ggplot( data ) +
     geom_histogram( aes( x = training_hours ), bins = 30, color = "blue", fill = "lightblue" )

ggplot( data = data ) +
  geom_boxplot( aes( x = is_looking_for_job, y = training_hours ), fill = c( "palevioletred1", "darkseagreen1" ) )
```
From the above plot we read that the training hours given to the candidates are positively skewed, with mean training hours of `r round(mean(data $ training_hours), 2)` hours. The distribution for both those candidates who want to stay with the company and those who do not show almost identical distribution. The box plot reports an approximate 50 hours of training time for both target variable groups, meaning that half of the candidates finished under 50 hours, while the rest needed more time to complete the course. 

The box plot also identifies outliers beyond `r boxplot.stats(data $ training_hours) $ stats[5]` hours, which regards `r count(data, training_hours > boxplot.stats(data $ training_hours) $ stats[5])[2, 2]` values. Since it is `r round(count(data, training_hours > boxplot.stats(data $ training_hours) $ stats[5])[2, 2] / nrow(data), 3) * 100`% of the the total dataset, we need to delve into the outliers a bit better.

In the case of employee training, it usually takes anywhere between 3 months to 6 month. Given that the company did not specifically target any candidate group with specific prerequisits, candidates applied with diverse background from all walks of life. Therefore it should not be surprising that for a few candidates with no former experience in the field might take more time for the training to complete. Assuming that the candidate will spend 4 hours a day on the training, we see that the outliers spent at least `r boxplot.stats(data $ training_hours) $ stats[5] / 4` days (`r boxplot.stats(data $ training_hours) $ stats[5] / 80` month) with the training. Which in this context seems absolutely normal. Therefore we will not deem any values outliers identified by the boxplot as outliers.

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
ggplot( data ) +
     geom_histogram( aes( x = training_hours, fill=is_looking_for_job ), position="fill")

ggplot( data ) +
  geom_density( aes( x = training_hours, fill = is_looking_for_job ), alpha = 0.3 ) + 
  theme_new
```

When `training_hours` is plotted as a standardised graph with the target variable overlay, we roughly see that those who spend more than 200 hours with the training tend to look for a job less likely than those who completed the training under 200 hours. Since the graphical indication is not strong enough, we will test whether the turnover rate is lower among those who spend more than 200 hours on the training versus those who spent less 200 hours.

**Hypotheses:** 

\[
\bigg\{
\begin{matrix}
          H_0:  \pi_{training\_hours<200, yes}   \leq  \pi_{training\_hours>=200, yes} \\
          H_a:  \pi_{training\_hours<200, yes}   >  \pi_{training\_hours>=200, yes} \\
\end{matrix}
\]

**Test results:**
```{r}
# subset data to 2 groups, those who complete the training under 200 hours and beyond
training_less_200 = data[data $ training_hours < 200, ]
training_more_200 = data[data $ training_hours >= 200, ]

# get the turnover rate for the 2 subset
training_less_200_looking = sum( training_less_200 $ is_looking_for_job == "yes")
training_more_200_looking = sum( training_more_200 $ is_looking_for_job == "yes")

# get subset size
n_training_less_200 = nrow( training_less_200 )
n_training_more_200 = nrow( training_more_200 )

# carry out 2-sample test for equality of proportions with continuity correction
prop.test( x = c( training_less_200_looking, training_more_200_looking ), 
           n = c( n_training_less_200, n_training_more_200 ),
           alternative = "greater")
```

**Test conclusion:**

Given significance level of $\alpha = 0.05$ , and p-value = $0.0628\%$ < $\alpha = 5\%$, there is sufficient evidence found to reject the $H_0$ in favour for $H_a$, and we infer that the turnover rate among those who spent less than 200 hours on the training is significantly greater than those who completed the training beyond 200 hours.

Given the above hypothesis testing we confirm the predictive importance of `training_hours`, and hence we would expect the data mining algorithm to incorporate that into the model.

Furthermore, the graphical and numerical evidence shall suggest the company to extend the training course beyond 200 hours, so that itdecrease the turnover rate among new candidates.


## Investigate the `city_development_index` variable {.unlisted .unnumbered}

The [City Development Index](https://www.cdindex.net/en/methodology) (CDI) aims to compare cities objectively from a holistic perspective both at the international and national levels.

```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
ggplot( data = data ) +
  geom_boxplot( aes( x = is_looking_for_job, y = city_development_index ), fill = c( "palevioletred1", "darkseagreen1" ) )

ggplot( data ) +
     geom_histogram( aes( x = city_development_index ), bins = 30, color = "blue", fill = "lightblue" )
```

The above box plots indicate that for the 2 target variable groups the distribution is significantly different. For both groups the data is skewed negatively, meaning, that the data overrepresents those who come from a higher socioeconomic city. The box plot identifies a few outliers for those not looking for a job. These observations are towards the lower end of CDI, and are classified as such if they are below `r boxplot.stats(data $ city_development_index) $ stats[1]`. This regards in total, `r count(data, city_development_index < boxplot.stats(data $ city_development_index) $ stats[1])[2, 2]` observations. 

Given that in less developed cities (according to the methodology of CDI), the opportunity to access proper social, economic, educational and cultural facilities are limited. Poor CDI, does not necessarily allow for a candidate to develop an affinity towards Data Science and hence, it is rare to get applications from these cities. Therefore, if a candidate comes from such a city with such low CDI, and passes the training course, it is indeed an outlier. Therefore, we will treat these observations as outliers and impute them by values drawn randomly from an underlying distribution.

```{r}
whisker_lower = boxplot.stats(data $ city_development_index) $ stats[1]

data = mutate( data, city_development_index = ifelse( city_development_index < whisker_lower, NA, city_development_index ) ) 

data $ city_development_index = impute( data $ city_development_index, 'random' )

summary(data $ city_development_index)
```
Next, we look at the standardised plots 
```{r, fig.show = "hold", out.width = "50%", fig.align = 'default' }
ggplot( data ) +
     geom_histogram( aes( x = city_development_index, fill=is_looking_for_job ), position="fill")

ggplot( data = data ) + 
  geom_density( aes( x = city_development_index, fill = is_looking_for_job ), alpha = 0.3)
```

The above histogram and density plot clearly show a negative relationship between turnover rate and CDI. Meaning, that the higher the CDI of the city the candidate comes from, the lower the likeliness that the candidate will look for another job after the training, implying a decrease in turnover rate.

Given the clear graphical evidence of the predictive importance of `city_development_index`, we expect the data mining algorithm to incorporate into the prediction model.


## Conclusion of EDA {.unlisted .unnumbered}

In EDA, we explored the following variables; `r names(data)`.

Based on the above analysis, our conclusion is the following:
* `city`: The variable does not bear with any predictive importance, nor does it provide with valuable information for the company
* `city_development_index`: The variable has a strong predictive capability. The higher CDI a candidate comes from, the lower the chance the candidate will look for a new job -  company needs to focus on candidates who come from high CDI cities.
* `gender`: The variable bears with predictive importance, and the company needs to focus on the male gender more, as they tend to retain their position in the largest fraction compared to the other genders.
* `relevant_experience`: The predictive importance of this variable is strong. The company needs to focus on candidates with relevant former experience as they stay in a larger percent of the cases than those without.
* `enrolled_university`: The variable has predictive capabilities. The company should look out for those with full time degree, as this group tend to change job about 37.5% of the cases.
* `education_level`: The variable has strong predictive importance. The company should specifically focus on candidates with at least a masters level degree.
* `major_discipline`: The variable has predictive capabilities. The company need to focus on groups with major discipline of Art, Humanities and those of unknown discipline, as they are the most loyal to the company after hiring.
* `experience`: The variable shows strong predictive importance. The relationship is clear, the more experience a candidate has, the less likely to leave the job. The company needs to focus on at least medior  but more of the senior candidates.
* `company_size`: The variable shows predictive capabilities, and the company needs to avoid candidates with unknown company size, as they tend to leave the job approximately 37.5% of the cases.
* `company_type`: The variable shows strong enough graphical indication of predictive importance. The company needs to look out for with unknown company type, as these candidates leave approximately 37.5% of the cases.
* `last_new_job`: The variable shows strong indication of predictive capabilities. The more a candidate stays at his/her current position, the less likely he/she will leave the job offered by the hiring company. Therefore, the company needs to focus on those, who are in their current position for longer than 4 years.
* `training_hours`: The variable has strong predictive importance. The company needs to focus on candidates with at least 200 hours spent on training hours.

# Data Preparation

In this section we prepare the dataset for modelling. Here, we partition the dataset at hand randomly into two groups: train set (80%) and test set (20%).

```{r}
set.seed( 5 ) # set seed for reproducibility

data_sets = partition( data = data, prob = c( 0.8, 0.2 ) )

train_set = data_sets $ part1
test_set  = data_sets $ part2

actual_test  = test_set $ is_looking_for_job
```

Next, we validate the partition by testing whether the proportion of the target variable `is_looking_for_job` differs between the partitions. We use a Two-Sample Z-Test for the difference in proportions. A Two-sample z-test is suitable here, because we want to compare the proportion of candidates who passed the trainig and want to leave the compayn between the two groups (“training set” and “test set”). The hypotheses are as follows

\[
\bigg\{
\begin{matrix}
          H_0:  \pi_{looking\_for\_job,\ train}   =  \pi_{looking\_for\_job,\ test} \\
          H_a:  \pi_{looking\_for\_job,\ train} \neq \pi_{looking\_for\_job,\ test}
\end{matrix}
\]

To validate the partitions, we run a hypothesis test to check the proportion of the target variable `churn` in the train and test sets. To run the test, we use the `prop.test` function in **R**:
```{r}
x1 = sum( train_set $ is_looking_for_job == "Yes" )
x2 = sum( test_set  $ is_looking_for_job == "Yes" )

n1 = nrow( train_set )
n2 = nrow( test_set  )

prop.test( x = c( x1, x2 ), n = c( n1, n2 ) )
```

Given significance level of $\alpha = 5%$ , and $p-value = 73.03%$ > $\alpha = 5\%$, there is insufficient evidence found to reject the $H_0$, and hence we infer that the two groups, training set and test set, are not different statistically. This indicates that the partition for the target variable is valid.


# Modeling - Classification

Per concluded above in the [Exploratory Data Analysis](#eda) section, the following predictors are relevant for the current dataset: `voice.plan`, `voice.messages`, `intl.plan`, `intl.mins`, `intl.calls`, `day.mins`, `eve.mins`, `night.mins`, and `customer.calls`. 
