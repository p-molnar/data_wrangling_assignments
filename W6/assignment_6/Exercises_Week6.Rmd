---
title: '<center> The Exercises of Week 6 - Regression Analysis <center>'
author: '<center> Reza Mohammadi <center>'
date: '<center> `r Sys.Date()` <center>'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 5
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include = FALSE}
knitr::opts_chunk $ set( echo = TRUE, message = FALSE, warning = FALSE, 
                         comment = " ", error = FALSE, fig.align = 'center'  )
```

```{css, echo=FALSE}
.answer_panel { margin-left: 40px; }

.sub_details { margin-left: 60px; }
```

**Full Name:**

* Peter Molnar (11759216)
  
**Online Assignment**: There is a new online-assignment at the [DataCamp](https://www.datacamp.com) with the name “*Supervised Learning in R: Regression*”. *The online-assignments at the DataCamp are not mandatory*.

**Your task** is to answer the following questions in this R-markdown file. Please upload both your R-markdown (.Rmd file) and the HTML files separately on Canvas. 

We want to apply regression models to analyze three different data sets. For this task, we are going to using the following **R** packages:

* **liver**: the *marketing*, *churn*, and *house* data sets are available in this package. 
* **ggplot2**: for plotting. 
* **psych**: for histograms and correlations for a data matrix.

If it’s needed, install these packages on your computer. Here we load them:

```{r}
library( liver )     

library( ggplot2 )     

library( psych )          
```

# Linear Regression Analysis (30 points)

We want to apply linear regression models to analyze a dataset which is called *marketing* and it is available in the [**liver**](https://CRAN.R-project.org/package=liver) package. Basically, we want to apply a simple linear regression to look at *what happens to our revenue when we spend more on Pay-per-click (PPC)*. 

## *Marketing* dataset

The *marketing* dataset contains 8 features and 40 records as 40 days that report how much we spent, how many clicks, impressions and transactions we got, whether or not a display campaign was running, as well as our revenue, click-through-rate and conversion rate. The target feature is revenue and the remaining 7 variables are predictors.

The *marketing* dataset, as a data frame, contains 40 records (rows) with 8 variables/features (columns). The variables are:

* `spend`: daily send of money on PPC (apy-per-click).
* `clicks`: number of clicks on for that ad.
* `impressions`: amount of impressions per day.
* `display`: whether or not a display campaign was running.
* `transactions`: number of transactions per day.
* `click.rate`:  click-through-rate.
* `conversion.rate`: conversion rate.
* `revenue`: daily revenue.

We import the *marketing* dataset and report the structure of the dataset:

```{r}
data( marketing, package = "liver" )

str( marketing )
```

It shows the dataset contains `r nrow( marketing )` records and `r ncol( marketing )` variables/features. The dataset has `r ncol( marketing ) - 1` predictors along with a target variable `revenue` as a numerical-continuous variable.

Below is a simple visualization of all the variables:
```{r, fig.height = 11, fig.width = 12}
pairs.panels( marketing )
```

The above plot presents bi-variate scatter plots (bottom-left), histograms (diagonals), and correlations (upper-right). 

a. **For each variable in the marketing dataset, specify its type.**

<details class="answer_panel">
  <summary>Answer a</summary>
  
| Variable| Statistical type | R object type | 
| :- | :-  | :- | 
| `spend` | numerical, continuous | num |   
| `clicks` | numerical, discrete | int |    
| `impressions` | numerical, discrete | int |   
| `display` | categorical, binary | int |   
| `transactions` | numerical, discrete | int |    
| `click.rate` | numerical, discrete$^*$  | num |    
| `conversion.rate` | numerical, discrete$^*$ | num |   
| `revenue` | numerical, discrete | num |   



$^*$ Because the underlying data that the percentages are calculated from is discrete

</details>

b. **What is your interpretation of the above plot? Provide your reasons.**

<details class="answer_panel">
  <summary>Answer b</summary>

The way the above plot is read is the following:

* the bottom diagonal half shows scatter plots the two variables where the plot intersects. For instance, the first scatter plot on the top left, plots observations of `spend` on the x-axis, and `clicks` on the y-axis. More specifically, the diagonal variable is always plotted as the independent variable (x-axis) and the vertical variable as dependent variable (y-axis).
* Along the diagonal line of the plot, we see histograms of the given variable, and it shows the distribution of it.
* on the upper diagonal half of the plot, we see a correlation matrix between the vertical and horizontal variables. For instance, the right most value, 0.79, stands for the linear correlation between `spend` and `revenue`.

</details>

c. **What is your interpretation of the relationship between variables `spend` and `clicks`? Provide your reasons.**

<details class="answer_panel">
  <summary>Answer c</summary>

The above plot shows strong positive correlation, $r_{spend, clicks} = 0.97$, between `spend` and `clicks`. This can be interpreted as, the more one spends on a campaign, the more clicks it is going to generate.

</details>

## Simple Linear Regression

By using simple linear regression, we want to estimate the *daily revenue* of the company given *daily send* of money on pay-per-click. Thus, we use a simple linear regression model to regress the variable `spend` (daily send of money on pay-per-click) on the target variable `revenue` (daily revenue). First, we report a scatter plot of the `spend` vs `revenue`, along with the least-squares regression line as follows

```{r}
ggplot( marketing, aes( x = spend, y = revenue ) ) +
    geom_point() +
    geom_smooth( method = "lm", se = FALSE ) +
    labs( title = "Daily Revenue Against Campaign Spend",
          x = "Daily spend (£)",
          y = "Daily revenue (£)" ) +
    theme_minimal() + ggtitle( "Scotor plot with Linear Regression line" )
```

To create the regression line, we used function `lm` which is for fitting linear models:

```{r}
reg_1 = lm( revenue ~ spend, data = marketing )
```

To know more about this function, type `?lm` in your Console. To see the summary of the regression results, we have
```{r}
summary( reg_1 )
```

a. **Give the estimated regression equation?**

<details class="answer_panel">
  <summary>Answer a</summary>

$\widehat{revenue}(spend) = 15.7058 + 5.2517*spend$

</details>

b. **What is the estimated value of the slope $\beta_1$? Explain clearly and interpret its value?**

<details class="answer_panel">
  <summary>Answer b</summary>

The estimated slope of $\beta_1$, which in this case stands paramatrically stands for `spend`, is 5.2517. It can be interpreted as if we spend an additional pound on the daily pay-per-click, then we expect revenue to increase on average by £5.2517.

</details>

c. **What would you conclude from the estimated *Residual standard error*?**

<details class="answer_panel">
  <summary>Answer c</summary>

The residual standard error measures the standard deviation of the residuals. In the current case, it is 93.82. Since it is a unitless and unstandardised measure of deviation, as a single metrics we cannot conlcude anything from it. If there were multiple competing models, then we could compare them based on their RSE values, and the most preferred would be the one with the lowest value.

</details>

d. **What would you conclude from the estimated *R-square*?**

<details class="answer_panel">
  <summary>Answer d</summary>

The $R^2$ measures the goodness of fit, and is interpreted as the the percentage of the variance in the dependent variable that the independent variables explains.
In this case $R^2 = 0.6232$ means, that $62.32\%$ of the variation in the `revenue` is explained by `spend`.

</details>

e. **Estimate the regression equation when `spend` is 25?**

<details class="answer_panel">
  <summary>Answer e</summary>

$\widehat{revenue}(25) = 15.7058 + 5.2517*25 = £146.9983$

</details>

f. **Estimate the regression equation when `spend` is 200?**

<details class="answer_panel">
  <summary>Answer f</summary>

$\widehat{revenue}(200) = 15.7058 + 5.2517*200 = £1066.0458$

</details>

g. **Verify whether it is reasonable to trust the estimated value in the previous part? Why? Explain your answer.**
  
<details class="answer_panel">
  <summary>Answer g</summary>

To varify the validaty of the estimates in e) and f), we report summary statistics of `spend`.

```{r}
summary(marketing $ spend)
```

Above we see that, `spend` ranges between £1.12 and £91.51.  Analysts should restrict estimates and predictions to the values within the range of the values of the independent variable in dataset. £25 is within the range, therefore, the prediction it gives is valid. However, £200 lies outside of the range, therefore the prediction may be overly erroneous.

</details>

## Multiple Linear Regression

The illustrate the multiple regression modeling using the *marketing* dataset, we shall add the predictor `display` to the model, and observe whether the quality of the model has improved or not. In this case, the equation for the multiple regression with the predictors is:

$$ \hat{y} = b_0 + b_1 x_1 + b_2 x_2 $$
Therefore, by using function `lm` we have:
```{r}
multi_reg = lm( revenue ~ spend + display, data = marketing )

summary( multi_reg )
```

a. **Give the estimated regression equation?**

<details class="answer_panel">
  <summary>Answer a</summary>

$\widehat{revenue}(spend, display) = -41.4377 + 5.3556*spend + 104.2878*display $

</details>

b. **What would you conclude from the estimated *Residual standard error*?**

<details class="answer_panel">
  <summary>Answer b</summary>
  
The residual standard error measures the standard deviation of the residuals. In the current case, it is 78.14. Since it is a unitless and unstandardised measure of deviation, as a single metrics we cannot conclude anything from it. However, in this case we can compare it with the simple linear regression. When comparing $RSE_{multi} = 78.14$ with $RSE_{simple} = 93.82$, we conclude that the multivariate linear regression's RSE value is smaller than that of simple linear regression's. That is, the additional variable, `display`, bears with relevant explanatory power of revenue.

</details>

c. **What would you conclude from the estimated *R-square*?**

<details class="answer_panel">
  <summary>Answer c</summary>

Given that the above output is for a multivariate linear regression, we need to take into account that any additional variable will incrase the regular $R^2$ value. Therefore, we need to look at the adjusted r-squared value that is a corrected for the number of independent variable in the model.

$R^2_{adjusted} = 0.7317$, is interpreted as follows; 73.17% of the variation in revenue is explained by the model.

</details>

d. **Compare the estimated regression model from this question with the estimated regression model from the previous part. Which one do you recommend for this dataset? Support your claim.** 

<details class="answer_panel">
  <summary>Answer d</summary>

Below we compare the estimates for $spend = 25$, since this value is within the range of `spend` and hence does not run the risk of extrapolation.

Regression equation from the previous part:
$\widehat{revenue}(25) = 15.7058 + 5.2517*25 = £146.9983$

Regression equation from this part:
$\widehat{revenue}(spend = 25, display=0) = -41.4377 + 5.3556*25 + 104.2878*0 = £92.4523$

We see, that the second regression provides a more modest revenue estimation than that of simple linear regression.

Since the adjusted r-squared value is larger and the RSE value is smaller for the multivariate model than for the simple linear model, we prefer the multivariate model.

</details>
  
## Model Specification: Choosing the Correct Regression Model

In the *marketing* dataset, we have only 7 predictors. But, most of business projects use dozens if not hundreds of predictors. We therefore need a method to ease the selection of the best regression model. This method is called *stepwise regression*. In stepwise regression, helpful predictors are entered into the model one at a time, starting with the most helpful predictors. Because of multicollinearity or other effects, when several helpful variables are entered, one of them may no longer be considered helpful any more, and should be dropped. For this reason, stepwise regression adds the most helpful predictors into the model and at a time and then checks to see if they all still belong. Finally, the stepwise algorithm can find no further helpful predictors and converges to a final model. 

To apply *stepwise regression* to the *marketing* dataset, first and foremost, we should build a linear model with all the available predictors included, so that we can have an understanding of the model, as well as to use the result of this model in the upcoming model selections. `revenue ~ .` inside the `lm()` function means the linear model includes all the columns in the data as predictors other than price.

```{r}
ml_all = lm( revenue ~ ., data = marketing )

summary( ml_all )
```

**What is your interpretation of this model? Should we keep all the predictors in our model? Support your claim.**

<details>
  <summary>Answer</summary>

The RSE and the r-squared values are similar to the model in 1.3. However, the difference is that, in the current model none of the independent variables are significant. Meaning, that none of the predictors differ significantly from 0, and therefore, does not significantly add to the model.

One remedy is to eliminate some independent variables and see how it changes the regression output. 
</details>
   
### Stepwise Regression

For *stepwise regression*, the function `step()` should be called and the direction is set to `both` so that the algorithm can add and drop predictors in every iteration. Once it is called, the iterating process will proceed by itself.

From the summary of the first iteration where we include all possible predictors, we can see that the model dropped `spend`, which is the predictor with the highest P-value in this model. As a result, in the second iteration when we analyze the impact of each predictor, the variable `spend` has a plus sign instead of a minus sign in front of it, meaning the impact is measured when the variable `spend` is added to our model.

Finally, when dropping/adding any variable will not give a positive impact to our model in terms of performance, the stepwise process is done.


```{r}
ml_step = step( ml_all, direction = "both" )
```

The see the selected regression model based on the *stepwise regression*, we can use function `summary()` as follows
```{r}
summary( ml_step )
```

**Compare the estimated regression model from this section with the estimated regression models from the previous parts (part 1.2 and part 1.3). Which one do you recommend for this dataset? Support your claim.**

<details>
  <summary>Answer</summary>

| formula | revenue ~ spend + display *(model 1)* | revenue ~ clicks + display *(model 2)*|
| :- | :-: | :-: |
| RSE | 78.142 | 72.29 |
| $R^2_{adjusted}$ | 0.7317  |  0.7704 |
| Model p-value | 1.012e-11 | 5.682e-13 |

Based on the above metrics, we can conclude the following:

* In terms of RSE, the model 2 produces less residuals than model 1. Between competing models, the one with the smaller RSE is preferred, therefore, model 2 is deemed a more accurate model.
* With regards to $R^2_{adjusted}$, model explains a larger percentage of the variation in `revenue`, and hence has higher explanatory power, therefore, again model 2 is preferred to model 1
* lastly, the model p-value is both highly significant for the 2 models, however, for model 2 it is more significant than for model 1.

We can concludes that the the differences are small between the competing models; it is because the correlation between `spend` and `clicks` is 0.97. Therefore, it was expected that the two models would produce similar results, but since model 2 outperforms in all metrics model 1, we deem model 2 a superior model.

</details>
   
## Verifying Model Assumptions

Before a model can be implemented, the requisite model assumptions must be verified. Using a model whose assumptions are not verified is like building a house whose foundation may be cracked. Making predictions using a model where the assumptions are violated may lead to erroneous and overoptimistic results, with costly consequences when deployed.

These assumptions are:

* *linearity*, 
* *independence*, 
* *normality*,  
* *constant variance*. 

We may be checked using the above assumptions by the follow plots

```{r, fig.show = "hold", fig.align = 'default', out.width = "50%"}
lm_select = lm( revenue ~ display + clicks, data = marketing )

plot( lm_select )  
```

As you can see in the Normal Q-Q plot (upper-right), the bulk of the points in the normal probability plot do line up on a straight line, so our *normality assumption* is essentially.

The plot of the residuals versus the fits (upper-left) is examined for discernible patterns. If obvious curvature exists in the scatter plot, the linearity assumption is violated. If the vertical spread of the points in the plot is systematically nonuniform, the constant variance assumption is violated. We detect no such patterns in the plot of the residuals versus fits and therefore conclude that the *linearity* and *constant variance* assumptions are intact for this example.

The *independence* assumption makes sense for this data set, since we would not expect that the revenue for one particular day would depend on the revenue for another day.

# Generalized Regression Analysis (20 points)

We want to apply generalized regression models to analyze the [*churn*](https://rdrr.io/cran/liver/man/churn.html) dataset that is available in the **R** package [**liver**](https://CRAN.R-project.org/package=liver). We import the *churn* dataset and report the structure of the dataset:

```{r}
data( churn )

str( churn )
```

It shows the dataset contains `r nrow( churn )` records and `r ncol( churn )` variables/features. The dataset has `r ncol( churn ) - 1` predictors along with a target variable `churn` as a binary variable.

## Logistic Regression 

By using *logistic regression* we want to classify whether or not a customer leaving the service of one company in favor of another company. 

The *churn* dataset has `r ncol( churn ) - 1` predictors but we are not going to use all of the predictors. We know based on the lecture of week 2, we should use only the predictors that have a relationship with the target variable. So, here we use the following predictors:

`account.length`, `voice.plan`, `voice.messages`, `intl.plan`, `intl.mins`, `day.mins`, `eve.mins`, `night.mins`, and `customer.calls`.

We use the following *formula* to list response (`churn`) the above predictors:
```{r}
formula = churn ~ account.length + voice.messages + day.mins + eve.mins + 
                  night.mins + intl.mins + customer.calls + intl.plan + voice.plan
```

To run the logistic regression model, we will use the `glm()` command:
```{r}
logreg_1 = glm( formula, data = churn, family = binomial )
```

The `formula` input response (`churn`) the above predictors, and the `data = churn` input specifiies the dataset, and `family = binomial` specifies a logistic regression model. Save the model output as `logreg_1`.

To view the summary of the model, run the `summary()` command with the name of the saved model as the sole input. 
```{r}
summary( logreg_1 )
```

**Based on the above output, which of the predictors should we remove from our model? Support you claim. Support your claim.**

<details>
  <summary>Answer</summary>

From the above output we see that `account.length` is not significantly different from 0, therefore its contribution to explain the target variable is questionable. Also, the variable `voice.messages`, is only significant at $\alpha > 5\%$, therefore, we might also deem this variable as not explanatory enough.

It is advised to eliminate `account.length` first and see how it impacts the rest of the variables. It might be the case that it improves the significance of `voice.messages`, and then it needs not to be eliminated. However, if this is not the case, `voice.messages` may be eliminated as well just to see if it improved the overall model's predictive power.


</details>

## Poisson Regression 

Poisson regression is used when we want to predict a count of events, such as how many times a customer will contact customer service. The distribution of the response variable will be a count of occurrences, with a minimum value of zero.

Here we want to use the *churn* dataset to build that estimate the number of customer service calls based on the following predictors:

`churn`, `account.length`, `voice.plan`, `voice.messages`, `intl.plan`, `intl.mins`, `day.mins`, `eve.mins`, and `night.mins`.

We use the following *formula* to list response (`customer.calls`) the above predictors:
```{r}
formula = customer.calls ~ churn + voice.messages + day.mins + eve.mins + 
                           night.mins + intl.mins + intl.plan + voice.plan
```
Not that the above `formula` specifies `customer.calls` as the response variable and the above variables as the predictor variables.

Our variable is an integer-valued variable, which is why we use *Poisson regression* instead of linear regression for this estimation. We use the `glm()` command, which we used in the previous section to build a logistic regression model, to now build a *Poisson regression* model:
```{r}
reg_pois = glm( formula, data = churn, family = poisson )
```

The `formula` input response (`customer.calls`) the above predictors, and the `data = churn` input specifies the dataset, and `family = poisson` specifies a *Poisson regression* model. Save the model output as `reg_pois`.

To view the summary of the model, run the `summary()` command with the name of the saved model as the sole input. 
```{r}
summary( reg_pois )
```

**Based on the above output, which of the predictors should we remove from our model? Support you claim. Support your claim.**

<details>
  <summary>Answer</summary>

From the above output we see that the contribution of `voice.messages`, `night.mins`, `voice.planno` are not significant. Furthermore, we see that the `intl.mins`, is only significant at $\alpha > 10\%$ and `eve.mins` at $\alpha > 5\%$.

It is advised to eliminate all non-significant variables first and see how it impacts the rest of the variables. If it does not change the latter two variables significance, then they may as well be eliminated if it improves the model's accuracy.

</details>


# Nonlinear Regression Analysis (30 points)

We want to apply *nonlinear* regression models to analyze a dataset which is called *house* and it is available in the [**liver**](https://CRAN.R-project.org/package=liver) package. Basically, we want to apply a *nonlinear regression* to estimate price of houses. 

## *house* dataset

The *house* dataset contains 6 features and 414 records. The target feature is `unit.price` and the remaining 5 variables are predictors. The variables are:

* `house.age`: house age (numeric, in year).
* `distance.to.MRT`: distance to the nearest MRT station (numeric).
* `stores.number`: number of convenience stores (numeric).
* `latitude`: latitude (numeric).
* `longitude`: longitude (numeric).
* `unit.price`: house price of unit area (numeric).

We import the *house* dataset and report the structure of the dataset:

```{r}
data( house )

str( house )
```

It shows the dataset contains `r nrow( house )` records and `r ncol( house )` variables/features. The dataset has `r ncol( house ) - 1` predictors along with a target variable `unit.price` as a numerical-continuous variable.

Below is a simple visualization of all the variables:
```{r, fig.height = 10, fig.width = 10}
pairs.panels( house )
```

The above plot presents bivariate scatter plots (bottom-left), hisgograms (diagonals), and correlations (upper-right). 

## Simple Linear Regression

Here we first apply *simple linear regression* then in the next section we will appy nonlinear regression.
By using simple linear regression, we want to estimate the *unit price* of the houses given *age* of the houses. Thus, we use a simple linear regression model to regress the variable `house.age`  on the target variable `unit.price`. First, we report a scatter plot of the `house.age` vs `unit.price`, along with the least-squares regression line as follows

```{r}
ggplot( data = house, aes( x = house.age, y = unit.price ) ) +
    geom_point() +
    geom_smooth( method = "lm", se = FALSE ) +
    labs( title = "Age of the houses against unit price of the houses",
          x = "House age (year)",
          y = "Unit Price ($)" ) +
    theme_minimal() + ggtitle( "Scotor plot with Linear Regression line" )
```

To create the regression line, we used function `lm()` which is for fitting linear models:

```{r}
reg_1 = lm( unit.price ~ house.age, data = house )
```

To see the summary of the regression results, we have
```{r}
summary( reg_1 )
```

a. **What would you conclude from the estimated *Residual standard error*?**

b. **What would you conclude from the estimated *R-square*?**

## Noninear Regression

By using nonlinear regression, we want to estimate the *unit price* of the houses given *age* of the houses. Thus, we use a simple linear regression model to regress the variable `house.age`  on the target variable `unit.price`. First, we report a scatter plot of the `house.age` vs `unit.price`, along with the least-squares regression line as follows

```{r}
ggplot( data = house, aes( x = house.age, y = unit.price ) ) +
    geom_point() + 
    stat_smooth( method = "lm", formula = y ~ x + I( x ^ 2 ), se = FALSE ) +
    theme_minimal() + ggtitle( "Scotor plot with Nonlinear Regression line" )
```

To create the *nonlinear regression*, we used function `lm()` which is for fitting linear models:
```{r}
reg_nonlinear = lm( unit.price ~ poly( house.age, 2 ), data = house[ -271, ] )
```

To see the summary of the regression results, we have
```{r}
summary( reg_nonlinear )
```

a. **What would you conclude from the estimated *Residual standard error*?**

b. **What would you conclude from the estimated *R-square*?**

c. **Compare the estimated regression model from this question with the estimated regression model from the previous part. Which one do you recommend for this dataset? Support your claim.** 

## Model Specification: Choosing the Correct Regression Model

We want here to choose the best regression model, by applying *stepwise regression* to the *house* dataset. First, we should build a regression model with all predictors included, so that we can have an understanding of the model, as well as to use the result of this model in the upcoming model selections:

```{r}
reg_all = lm( unit.price ~ poly( house.age, 2 ) + distance.to.MRT + stores.number + 
                           latitude + longitude, data = house )

summary( reg_all )
```

### Stepwise Regression

For *stepwise regression*, the function `step()` should be called and the direction is set to `both` so that the algorithm can add and drop predictors in every iteration. Once it is called, the iterating process will proceed by itself.

```{r}
reg_step = step( reg_all, direction = "both" )
```

The see the selected regression model based on the *stepwise regression*, we can use function `summary()` as follows
```{r}
summary( reg_step )
```

**Compare the estimated regression model from this section with the estimated regression model from the previous parts. Which one do you recommend for this dataset? Support your claim.** 

# Classification for churn dataset (20 points)

We want to apply the decision tree analysis to the [*churn*](https://rdrr.io/cran/liver/man/churn.html) dataset that is available in the **R** package [**liver**](https://CRAN.R-project.org/package=liver). We want to classify whether or not a customer leaving the service of one company in favor of another company. 

The *churn* dataset has 19 predictors but we are not going to use all of the predictors. We know based on the lecture of week 2, we should use only the predictors that have a relationship with the target variable. So, here we use the following predictors:

`account.length`, `voice.plan`, `voice.messages`, `intl.plan`, `intl.mins`, `day.mins`, `eve.mins`, `night.mins`, and `customer.calls`.

For this classification task, we want to apply the following algorithms:

* *Logistic Regression* algorithm (similar to part 2.1),
* *CART* algorithm (the same as weekly exercises of week 5),
* *C5.0* algorithm (the same as weekly exercises of week 5),
* *Random Forest* algorithm (the same as weekly exercises of week 5),
* *kNN* algorithm (the same as weekly exercises of week 5).

Ultimately, **we want to see which of the above classification algorithms are more suitable here**, by evaluating the accuracy of the predictions with:

* Confusion Matrix,
* MSE,
* ROC curve,
* AUC (Area Under the ROC curve).

## Data Preparation

After importing the *churn* dataset in R, partition the *churn* dataset randomly into two groups as a train set (80%) and a test set (20%). Here, for the partition, you could use the `partition()` function from the *liver* package. You should use the `set.seed()` function in R; similar to the data preparation in section 1 for adult dataset.

## Applying Logistic Regression algorithm

By using logistic regression we want to classify whether or not a customer leaving the service of one company in favor of another company. You the code from part 2.1.

## Applying Decision Tree algorithms

For the classification task, by using the training dataset, run the following algorithms:

* *CART algorithm* using the `rpart()` function in the **rpart** R package and plot the decision tree using the `rpart.plot()` function in the **rpart.plot** R package; Similar to the weekly exercises of week 5.

* *C5.0 algorithm* using the `C5.0()` function in the R package **C50**; Similar to the weekly exercises of week 5.

* *Random Forest* using the `randomForest()` function from the R package **randomForest**; Similar to the weekly exercises of week 5.

Based on the training dataset and the above models predict for the test set. For the prediction, you could use the `predict()` function; Similar to the weekly exercises of week 5.

## Applying kNN algorithm

Find the k-nearest neighbor for the test set, based on the training dataset, for the case k = 13. For the kNN algorithm, use min-max normalization to transfer the predictors, similar to the exercises of week 4.

## Model Evaluation

Based on your results so far, which of the four classification algorithms is more suitable here for the *churn* dataset based on:

* Confusion Matrix,
* MSE,
* ROC curve,
* AUC (Area Under the ROC curve).
  
# **Bonus**: Applying Regression Analysis for your own dataset (30 points)

In this part, similar to the above parts, apply the Regression Analysis approach for your own dataset. 




