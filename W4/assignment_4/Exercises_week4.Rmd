---
title: '<center> The Exercises of Week 4 - Classification and Model Evaluation <center>'
author: '<center> Reza Mohammadi <center>'
date: '<center> `r Sys.Date()` <center>'
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 7
    fig_height: 5
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include = FALSE}
knitr::opts_chunk $ set( echo = TRUE, message = FALSE, warning = FALSE, 
                         comment = " ", error = FALSE, fig.align = 'center'  )
```


```{css, echo=FALSE}
.answer_panel { margin-left: 40px; }

.sub_details { margin-left: 60px; }
```

**Full Name:**

* Peter Molnar (11759216)
  
**Online Assignment**: There is a new online assignment at the [DataCamp](https://www.datacamp.com) with the name “*Chapter 1: k-Nearest Neighbors (kNN)*” which is a part of the online course “Supervised Learning in R: Classification” at the DataCamp. *The online assignments at the DataCamp are not mandatory*.

**Your task** is to answer the following questions in *Part 1* and *Part 2* in this R-markdown file. Please upload both your R-markdown (.Rmd file) and the HTML files separately on Canvas. Note that your R-markdown (.Rmd file) and the HTML files have to be in the right format.

Here, we are going to use the following R packages:

* **[liver](https://CRAN.R-project.org/package=liver)**: the *bank* and *churn* datasets are in this package. We also use the `partition()`, `kNN()` functions in this package.
* **[ggplot2](https://ggplot2.tidyverse.org/index.html)**: we use this package for visualization.
* **[pROC](https://CRAN.R-project.org/package=pROC)**: to create ROC curve and compute AUC values, we use the `roc()` and `auc()` functions in this package.

If it’s needed, install these packages on your computer. Here we load them:
```{r}
if( !require( liver   ) ) install.packages( "liver" )
if( !require( ggplot2 ) ) install.packages( "ggplot2" )
if( !require( pROC    ) ) install.packages( "pROC" )

library( liver )  
library( ggplot2 )   
library( pROC )
library( caret )
```

# Classification for Bank direct marketing dataset (40 points)

By using different classification algorithms, we want to detect customers who are more likely to acquire the product.

## Business Understanding

Find the best strategies to improve for the next marketing campaign. How can the financial institution have greater effectiveness for future marketing campaigns? To make a data-driven decision, we need to analyze the last marketing campaign the bank performed and identify the patterns that will help us find conclusions to develop future strategies.

### Bank direct marketing info

Two main approaches for enterprises to promote products/services are: 

* *mass campaigns*: targeting general indiscriminate public,
* *directed marketing*, targeting a specific set of contacts. 

In general, positive responses to mass campaigns are typically very low (less than 1%). On the other hand, direct marketing focuses on targets that are keener to that specific product/service, making this kind of campaign more effective. However, direct marketing has some drawbacks, for instance, it may trigger a negative attitude towards banks due to the intrusion of privacy.

Banks are interested to increase financial assets. One strategy is to offer attractive long-term deposit applications with good interest rates, in particular, by using directed marketing campaigns. Also, the same drivers are pressing for a reduction in costs and time. Thus, there is a need for an improvement in efficiency: lesser contacts should be done, but an approximate number of successes (clients subscribing to the deposit) should be kept.

### What is a Term Deposit?

A Term Deposit is a deposit that a bank or a financial institution offers with a fixed rate (often better than just opening a deposit account), in which your money will be returned at a specific maturity time. For more information with regards to Term Deposits please check [here](https://www.investopedia.com/terms/t/termdeposit.asp).

## Data Undestanding

The *bank* dataset is related to direct marketing campaigns of a Portuguese banking institution. You can find more information related to this dataset at: [https://rdrr.io/cran/liver/man/bank.html](https://rdrr.io/cran/liver/man/bank.html)

The marketing campaigns were based on phone calls. Often, more than one contact (to the same client) was required, to access if the product (bank term deposit) would be (or not) subscribed. The classification goal is to predict if the client will subscribe to a term deposit (variable deposit).

We import the *bank* dataset:
```{r}
data( bank )      
```

We can see the structure of the dataset by using the `str()` function:
```{r}
str( bank )
```

It shows that the *bank* dataset as a `data.frame` has `r ncol( bank )` variables and `r nrow( bank )` observations. The dataset has `r ncol( bank ) - 1` predictors along with the target variable `deposit` which is a binary variable with 2 levels "yes" and "no". The variables in this dataset are:

* `age`: numeric.
* `job`: type of job; categorical: "admin.", "unknown", "unemployed", "management", "housemaid", "entrepreneur", "student", "blue-collar, "self-employed", "retired", "technician", "services".
* `marital`: marital status; categorical: "married", "divorced", "single"; note: "divorced" means divorced or widowed.
* `education`: categorical: "secondary", "primary", "tertiary", "unknown".
* `default`: has credit in default?; binary: "yes","no".
* `balance`: average yearly balance, in euros; numeric.
* `housing`: has housing loan? binary: "yes", "no".
* `loan`: has personal loan? binary: "yes", "no".

Related with the last contact of the current campaign:

* `contact`: contact: contact communication type; categorical: "unknown","telephone","cellular". 
* `day`: last contact day of the month; numeric.
* `month`: last contact month of year; categorical: "jan", "feb", "mar", ..., "nov", "dec".
* `duration`: last contact duration, in seconds; numeric.

Other attributes:

* `campaign`: number of contacts performed during this campaign and for this client; numeric, includes last contact.
* `pdays`: number of days that passed by after the client was last contacted from a previous campaign; numeric, -1 means client was not previously contacted.
* `previous`: number of contacts performed before this campaign and for this client; numeric.
* `poutcome`: outcome of the previous marketing campaign; categorical: "success", "failure", "unknown", "other".

Target variable:

* `deposit`: Indicator of whether the client subscribed a term deposit; binary: "yes" or "no".

Here we report the summary of the dataset:
```{r}
summary( bank )
```

## Data Preparation

We partition the *bank* dataset randomly into two groups: train set (80%) and test set (20%). Here, we use the `partition()` function from the *liver* package:

```{r}
set.seed( 5 )

data_sets = partition( data = bank, prob = c( 0.8, 0.2 ) )

train_set = data_sets $ part1
test_set  = data_sets $ part2

actual_test  = test_set $ deposit
```

Note that here we are using the `set.seed()` function to create reproducible results. 

We want to validate the partition by testing whether the proportion of the target variable `deposit` differs between the two data sets. We use a Two-Sample Z-Test for the difference in proportions. To run the test, we use the `prop.test()` function in **R**:
```{r}
x1 = sum( train_set $ deposit == "yes" )
x2 = sum( test_set  $ deposit == "yes" )

n1 = nrow( train_set )
n2 = nrow( test_set  )

prop.test( x = c( x1, x2 ), n = c( n1, n2 ) )
```

Based on the output, answer the following questions:

a. **Why is the above hypothesis test suitable for the above research question? Provide your reasons.**

<details class="answer_panel">
  <summary>Answer a</summary>
  
Above we want to validate the two partitions, training set and test set, to see if the test set bears with similar characteristics as the training set and conclude that it is representative of the dataset as a whole. Since the type of the target variable `deposit` is of binary/flag, we can compare the two subsets by conducting a two-sample Z-Test for the difference in proportions for depositors.
  
If the proportions are significantly different between training set and test set, then it may lead to bias or come at the cost of prediction/classification precision. Else, if the proportions are approximately the same, we can conclude that the training set is similar enough to represent the dataset as a whole and we can yield statistically meaningful results.

</details>

b. **Specify the null and alternative hypotheses?**

<details class="answer_panel">
  <summary>Answer b</summary>

\[
\bigg\{
\begin{matrix}
          H_0:  \pi_{training, deposit=yes}  = \pi_{test, deposit=yes} \\
          H_a:  \pi_{training, deposit=yes}  \neq \pi_{test, deposit=yes} \\
\end{matrix}
\]

</details>

c. **Explain that you reject or do not reject the null hypothesis, at $\alpha=0.05$. What would be your statistical conclusion?**

<details class="answer_panel">
  <summary>Answer c</summary>

**Statistical conclusion:** Given significance level of $\alpha = 0.05$, and $p-value = 0.1217 > \frac{\alpha}{2} = 0.025$, there is insufficient evidence found to reject $H_0$, and we infer that the population proportions of the number of people subscribed to the term deposit is not different between the training and the test partitions.

</details>

d. **What would be a non-statistical interpretation of your findings in c?**

<details class="answer_panel">
  <summary>Answer d</summary>

**Non-statistical conclusion:** At $\alpha = 0.05$, we infer that the population proportions of the number of people subscribed to the term deposit is not different between the training and the test partitions, and hence the training set is a representative partition of the dataset as a whole.

</details>


## Classification using the kNN algorithm

The results from the "Exploratory Data Analysis (EDA)" (from last week) indicate that the following predictors from `r ncol( bank ) - 1` predictors in the *bank* dataset are important to predict `deposit`.

`age`, `default`, `balance`, `housing`, `loan`, `duration`, `campaign`, `pdays`, and `previous`.

Thus, here, based on the training dataset, we want to apply kNN algorithm, by using above predictors in our model. We use the following formula:
```{r}
formula = deposit ~ age + default + balance + housing + loan + duration + campaign + pdays + previous
```

**NOTE:** The above formula means `deposit` is the target variable and the rest of the variables in the right side of tilde ("`~`") are independent variables. 

Based on the training dataset, we want to find the k-nearest neighbor for the test data set. Here we use two different values for k (k = 3 and k = 10). We use the `kNN()` function from the *R* package **liver**:
```{r}
predict_knn_3  = kNN( formula, train = train_set, test = test_set, k = 3  )

predict_knn_10 = kNN( formula, train = train_set, test = test_set, k = 10 )
```

To have an overview of the prediction result, we report *Confusion Matrix* for two different values of k by using the `conf.mat` function: 
```{r}
( conf_knn_3 = conf.mat( predict_knn_3, actual_test ) )

( conf_knn_10 = conf.mat( predict_knn_10, actual_test ) )
```

We also could report *Confusion Matrix* by using the `conf.mat.plot()` command:
```{r fig.show = "hold", out.width = "50%", fig.align = 'default'}
conf.mat.plot( predict_knn_3, actual_test, main = "kNN with k = 3" )

conf.mat.plot( predict_knn_10, actual_test, main = "kNN with k = 10" )
```

**What do these values mean? Explain what conclusion you will draw.**

<details>
  <summary>Answer</summary>
  
* There are 2 possible predicted classes: "yes" and "no"
  * "yes" meaning that we predicted that the targeted person would subscribe to the term deposit
  * "no" meaning that we predicted that the targeted person would not subscribe to the term deposit
* The classifier made a total of 946 predictions

<h3>k=3</h3>

The classifier 

* predicted "no" for 882 people
* predicted "yes" for 64 people
* correctly predicted the outcome 806 + 19 = 825 times
* incorrectly predicted the outcome 45 + 76 = 121 times
  
Below we report a list of model evaluation ratios to draw a conclusion:
```{r}
confusionMatrix(data = predict_knn_3, reference = actual_test)
```
We identify the following:

* **Accuracy**: overall the classifier is correct 87.21% of the time
* **Misclassification rate**: overall the classifier is wrong 12.79% of the time
* **Sensitivity**: the model correctly classifies positive records 94.71% of the time
* **Specificity**: the model correctly classifies negative records 20.00% of the time

<h3>k=10</h3>

The classifier 

* predicted "no" for 908 people
* predicted "yes" for 38 people
* correctly predicted the outcome 834 + 21 = 855 times
* incorrectly predicted the outcome 17 + 74 = 91 times
  
Below we report a list of model evaluation ratios to draw a conclusion:
```{r}
confusionMatrix(data = predict_knn_10, reference = actual_test)
```
We identify the following:

* **Accuracy**: overall the classifier is correct 90.38% of the time
* **Misclassification rate**: overall the classifier is wrong 0.0962% of the time
* **Sensitivity**: the model correctly classifies positive records 98.00% of the time
* **Specificity**: the model correctly classifies negative records 22.11% of the time
</details>


<h3>Conclusion</h3>

Based on the rate of accuracy, sensitivity, and specificity, the kNN algorithm with k=10 deemed to be a better predictor.

## Model evaluation by MSE

To evaluate the accuracy of the predictions, we calculate the Mean Square Error (MSE) by using the `mse()` function from the **liver** package:

```{r}
MSE_3 = mse( predict_knn_3, actual_test  )
MSE_3 

MSE_10 = mse( predict_knn_10, actual_test )
MSE_10
```

**For the case k=3, the MSE = `r round( MSE_3 , 3 )` and for the case k = 10, the MSE = `r round( MSE_10, 3 )`. What do these values mean? Explain what conclusion you will draw.**

<details>
  <summary>Answer</summary>

The Mean Squared Error combines the variance and bias into one formula and measures the squared difference between the observed value and the predicted value of a continuous target variable. Given that in our case that target variable is binary, the interpretation may not hold accurately.

For k=3, MSE of 0.128 means that the on average our prediction is approximately 12.8% off.
For k=10, MSE of 0.096 means that the on average our prediction is approximately 9.6% off.

Between the two competing models, the lower the MSE, the better the forecast, hence kNN with k=10 is more optimal choice.
  
</details>

## Visualizing Model Performance by ROC curve

To report the ROC curve, we need the probability of our classification prediction. We can have it by using:
```{r}
prob_knn_3  = kNN( formula, train = train_set, test = test_set, k = 3 , type = "prob" )[ , 1 ]

prob_knn_10 = kNN( formula, train = train_set, test = test_set, k = 10, type = "prob" )[ , 1 ]
```

To visualize the model performance, we could report the ROC curve plot by using the `plot.roc()` function from the **pROC** package:

```{r}
roc_knn_3 = roc( actual_test, prob_knn_3 )

roc_knn_10 = roc( actual_test, prob_knn_10 )

ggroc( list( roc_knn_3, roc_knn_10 ), size = 0.8 ) + 
    theme_minimal() + ggtitle( "ROC plots with AUC for kNN") +
    scale_color_manual( values = c( "red", "blue" ), 
    labels = c( paste( "k=3 ; AUC=", round( auc( roc_knn_3  ), 3 ) ),
                paste( "k=10; AUC=", round( auc( roc_knn_10 ), 3 ) )
                ) ) +
    theme( legend.title = element_blank() ) +
    theme( legend.position = c( .7, .3 ), text = element_text( size = 17 ) )
```


In the above plot, '<span style="color:red">red</span>' curve is for the case k = 3 and the '<span style="color:blue">blue</span>' curve is for the case k = 10.

**Explain what conclusion you will draw. Do we need to report AUC (Area Under the Curve) as well?**

<details class="answer_panel">
  <summary>Answer</summary>
  
Given that the blue curve always lies above the red curve (except at the two extreme points of sensitivity and specificity) we can conclude that the blue curve must enclose a larger area under the curve than the red curve. Meaning, that AUC for k=10 must be greater than for k=3.

From the graph it is unequivocal, that the blue curve always lies above the red curve, hence AUC is not necessary to be reported.

</details>

## kNN algorithm with data transformation

The predictors that we used in the previous question, do not have the same scale. For example, variable `duration` change between `r min( bank $ duration )` and `r max( bank $ duration )`, whereas the variable `loan` is binary. In this case, the values of variable `duration` will overwhelm the contribution of the variable `loan`. To avoid this situation we use normalization. So, we use min-max normalization and transfer the predictors. 

Now, we find the k-nearest neighbor for the test set, based on the training dataset, for the k = 10:

```{r}
predict_knn_10_trans = kNN( formula, train = train_set, test = test_set, transform = "minmax", k = 10 )

conf.mat.plot( predict_knn_10_trans, actual_test )
```

## ROC curve and AUC for transformed data

To report the ROC curve, we need the probability of our classification prediction. We can have it by using:
```{r}
prob_knn_10 = kNN( formula, train = train_set, test = test_set, k = 10, type = "prob" )[ , 1 ]

prob_knn_10_trans = kNN( formula, train = train_set, test = test_set, transform = "minmax", k = 10, type = "prob" )[ , 1 ]
```

To visualize the model performance between the raw data and the transformed data, we could report the ROC curve plot as well as AUC (Area Under the Curve) by using the `plot.roc()` function from the **pROC** package:

```{r}
roc_knn_10 = roc( actual_test, prob_knn_10 )

roc_knn_10_trans = roc( actual_test, prob_knn_10_trans )

ggroc( list( roc_knn_10, roc_knn_10_trans ), size = 0.8 ) + 
    theme_minimal() + ggtitle( "ROC plots with AUC for kNN") +
    scale_color_manual( values = c( "red", "blue" ), 
      labels = c( paste( "Raw data             ; AUC=", round( auc( roc_knn_10 ), 3 ) ), 
                  paste( "Transformed data; AUC=", round( auc( roc_knn_10_trans ), 3 ) ) ) ) +
  theme( legend.title = element_blank() ) +
  theme( legend.position = c( .7, .3 ), text = element_text( size = 17 ) )
```

In the above plot black curve is for the *raw* dataset and the red curve is for the *transformed* dataset.

**Explain what conclusion you will draw. Based on the values of AUC (Area Under the Curve), explain what conclusion you will draw.**

<details class="answer_panel">
  <summary>Answer</summary>
  
In this part the different variables are standardised, and therefore no variable will overwhelm the contribution of others'. We observe, that that the transformation improved AUC by 0.017, resulting in in an AUC of 0.814.

Between competing models, the one with higher AUC is preferred, therefore, the transformed data is deemed a better predictor.

</details>

## Optimal value of k for the kNN algorithm

In the previous questions, for finding the k-nearest neighbor for the test set, we set k = 10. But why 10? Here, we want to find out the optimal value of k based on our dataset. 

To find out the optimal value of `k` based on *Error Rate*, for the different values of k from 1 to 30, we run the k-nearest neighbor for the test set and compute the *Error Rate* for these models, by running `kNN.plot()` command 

```{r}
kNN.plot( formula, train = train_set, test = test_set, transform = "minmax", 
          k.max = 30, set.seed = 7 )
```

**Based on the plot, what value of k is optimal? Provide your reasons.**

<details class="answer_panel">
  <summary>Answer</summary>
  
Between competing models, one with the low error rate with a relatively large k value is preferred. Since the differences in error rates on the above graph is small, an optimal k-value can be anywhere between 18 and 22.

</details>

# Applying kNN algorithm for *churn* dataset (60 points)

Apply the kNN algorithm to analyze the *churn* dataset which is available in the **R** package [**liver**](https://CRAN.R-project.org/package=liver). 

## Import the *churn* dataset

Import the *churn* dataset in R and report the summary of the dataset. 

<details class="answer_panel">
  <summary>Answer</summary>

```{r}
data( churn )
```

```{r}
summary( churn )
```

</details>


## Data Preparation

First, partition the *churn* dataset randomly into two groups as a train set (80%) and test set (20%). Then, validate the partition for a couple of variables; for example, you could validate the partition by testing whether the proportion of churners differs between the two datasets. Or you could validate the partition by testing whether the population means for the number of customer service calls differs between the two datasets.

<details class="answer_panel">
  <summary>Answer</summary>
<h3>Partitioning</h3>

Below we set a seed of 13 for the sake of reproducibility and partition the `churn` dataset into 2 groups; the training set consist of 80% of the original dataset, while the test set consist of the rest 20% of the data.

```{r}
set.seed( 13 )

data_sets = partition( data = churn, prob = c( 0.8, 0.2 ) )

train_set = data_sets $ part1
test_set  = data_sets $ part2

actual_test  = test_set $ churn
```

<h3>Partition validation</h3>

<h4>Test for the differences in proportions</h4>

Next, we validate the partitioned sets by means of hypothesis testing.

For that, first we prepare the variables necessary for a proportion test.

```{r}
x1 = sum( train_set $ churn == "yes" )
x2 = sum( test_set  $ churn == "yes" )

n1 = nrow( train_set )
n2 = nrow( test_set  )
```

Next, we validate the partition by testing whether the proportion of the target variable `churn` differs between the two data sets. We use a Two-Sample Z-Test for the difference in proportions. Additionally, we draw up the following hypotheses to test.

**Hypotheses:**
\[
\bigg\{
\begin{matrix}
          H_0:  \pi_{training, churn=yes}  = \pi_{test, churn=yes} \\
          H_a:  \pi_{training, churn=yes}  \neq \pi_{test, churn=yes} \\
\end{matrix}
\]

```{r}
prop.test( x = c( x1, x2 ), n = c( n1, n2 ) )
```

**Statistical conclusion:** Given significance level of $\alpha = 0.05$, and $p-value = 0.4832 > \alpha = 0.025$, there is insufficient evidence found to reject $H_0$, and we infer that the population proportions of the number of churners is not different between the training and the test partitions.

<h4>Test for the differences in means</h4>

Additionally, we validate the partition by testing whether the population means for the number of customer service calls differs between the two datasets. We use a Two-Sample t-Test for the difference in means.

```{r}
t.test( x = train_set $ customer.calls,
        y = test_set $ customer.calls,
        alternative = "two.sided", 
        conf.level = 0.95 )
```

**Hypotheses:** 
\[
\bigg\{
\begin{matrix}
          H_0:  \mu_{training, customer.calls}  = \mu_{test, customer.calls} \\
          H_a:  \mu_{training, customer.calls}  \neq \mu_{test, customer.calls} \\
\end{matrix}
\]


**Statistical conclusion:** Given significance level of $\alpha = 0.05$, and $p-value = 0.8956 > \frac{\alpha}{2} = 0.025$, there is insufficient evidence found to reject $H_0$, and we infer that there is no difference in population means of the number of customer services calls for churners between the 2 dataset.

<h3>Overall conclusion</h3>

Given, that both the differences in population proportions, and differences in population means failed to show significance, we can conclude that the training set is similar enough to the test set to represent the dataset as a whole.

</details>

## Applying the kNN algorithm using all predictors

Based on the training dataset, find the k-nearest neighbor for the test data set, for the case k = 10; For this question, use all the 19 predictors of the *churn* dataset for the analysis. Note that you should use min-max normalization and transfer the predictors. Then, evaluate the accuracy of the predictions by reporting 

* Confusion Matrix
* MSE
* ROC curve
* AUC

<details class="answer_panel">
  <summary>Answer</summary>

<details class="sub_details">
  <summary>KNN</summary>

First, we transfer the predictors of `churn` by using min-max.

```{r}
formula = churn ~ .
predict_knn_10_trans = kNN( formula, train = train_set, test = test_set, transform = "minmax", k = 10 )
```


</details>
<details class="sub_details">
  <summary>Confusion Matrix</summary>
  
```{r}
( conf_knn_10_trans = conf.mat( predict_knn_10_trans, actual_test ) )
conf.mat.plot( predict_knn_10_trans, actual_test, main = "Transferred data, kNN with k = 10" )
```

* There are 2 possible predicted classes: "yes" and "no"
  * "yes" meaning that we predicted that the person churned
  * "no" meaning that we predicted that the person would not churn
* The classifier made in total 1043 predictions
* predicted "no" for 1040 people
* predicted "yes" for 3 people
* correctly predicted the outcome 3 + 888 = 891 times
* incorrectly predicted the outcome 152 times
  
Below we report a list of model evaluation ratios to draw a conclusion:
```{r}
confusionMatrix(data = conf_knn_10_trans, reference = actual_test)
```
We identify the following:

* **Accuracy**: overall the classifier is correct 85.43% of the time
* **Misclassification rate**: overall the classifier is wrong 14.57% of the time
* **Sensitivity**: the model correctly classifies positive records 1.94% of the time
* **Specificity**: the model correctly classifies negative records 100% of the time  
  
</details>

<details class="sub_details">
  <summary>MSE</summary>
  
```{r}
MSE_10_trans = mse( predict_knn_10_trans, actual_test )
MSE_10_trans 
```
The Mean Squared Error combines the variance and bias into one formula and measures the squared difference between the observed value and the predicted.

The above MSE of 0.1457 means that the on average our prediction for churning is approximately 14.57% off.
</details>

<details class="sub_details">
  <summary>ROC & AUC Curve</summary>

```{r}
prob_knn_10_trans = kNN( formula, train = train_set, test = test_set, transform = "minmax", k = 10, type = "prob" )[ , 1 ]

roc_knn_10_trans = roc( actual_test, prob_knn_10_trans )

ggroc( list( roc_knn_10_trans ), size = 0.8 ) + 
    theme_minimal() + ggtitle( "ROC plots with AUC for kNN") +
    scale_color_manual( values = c( "red" ), 
    labels = c( paste( "k=10; AUC=", round( auc( roc_knn_10_trans ), 3 ) )
                ) ) +
    theme( legend.title = element_blank() ) +
    theme( legend.position = c( .7, .3 ), text = element_text( size = 17 ) )

```
  
The area under the curve is 0.356.
  
</details>


</details>

## Applying the KNN algorithm using part of predictors

In the previous exercises for the *churn* dataset, you suppose to use all the 19 predictors for the analysis. But we know based on the lecture of week 2, we should use only those predictors that have a relationship with the target variable. So, here we use the following predictors:

`account.length`, `voice.plan`, `voice.messages`, `intl.plan`, `intl.mins`, `day.mins`, `eve.mins`, `night.mins`, and `customer.calls`.

First, based on the above predictors, find the k-nearest neighbor for the test set, based on the training dataset, for the k = 10. Note that you should use min-max normalization and transfer the predictors. Then, evaluate the accuracy of the predictions by reporting 

* Confusion Matrix
* MSE
* ROC curve
* AUC

<details class="answer_panel">
  <summary>Answer</summary>

<details class="sub_details">
  <summary>KNN</summary>

First, we transfer the predictors of `churn` by using min-max.

```{r}
formula = churn ~ account.length + voice.plan + voice.messages + intl.plan + intl.mins + day.mins + eve.mins + night.mins + customer.calls
predict_knn_10_trans = kNN( formula, train = train_set, test = test_set, transform = "minmax", k = 10 )
```


</details>
<details class="sub_details">
  <summary>Confusion Matrix</summary>
  
```{r}
( conf_knn_10_trans = conf.mat( predict_knn_10_trans, actual_test ) )
conf.mat.plot( predict_knn_10_trans, actual_test, main = "Transferred data, kNN with k = 10" )
```

* There are 2 possible predicted classes: "yes" and "no"
  * "yes" meaning that we predicted that the person churned
  * "no" meaning that we predicted that the person would not churn
* The classifier made in total 1043 predictions
* predicted "no" for 971 people
* predicted "yes" for 72 people
* correctly predicted the outcome 62 + 878 = 940 times
* incorrectly predicted the outcome 93 + 10 = 103 times
  
Below we report a list of model evaluation ratios to draw a conclusion:
```{r}
confusionMatrix(data = conf_knn_10_trans, reference = actual_test)
```
We identify the following:

* **Accuracy**: overall the classifier is correct 90.12% of the time
* **Misclassification rate**: overall the classifier is wrong 9.88% of the time
* **Sensitivity**: the model correctly classifies positive records 40% of the time
* **Specificity**: the model correctly classifies negative records 98.87% of the time  
  
</details>

<details class="sub_details">
  <summary>MSE</summary>
  
```{r}
MSE_10_trans = mse( predict_knn_10_trans, actual_test )
MSE_10_trans 
```
The Mean Squared Error combines the variance and bias into one formula and measures the squared difference between the observed value and the predicted.

The above MSE of 0.0997 means that the on average our prediction for churn rate is approximately 9.97% off.

</details>

<details class="sub_details">
  <summary>ROC & AUC Curve</summary>

```{r}
prob_knn_10_trans = kNN( formula, train = train_set, test = test_set, transform = "minmax", k = 10, type = "prob" )[ , 1 ]

roc_knn_10_trans = roc( actual_test, prob_knn_10_trans )

ggroc( list( roc_knn_10_trans ), size = 0.8 ) + 
    theme_minimal() + ggtitle( "ROC plots with AUC for kNN") +
    scale_color_manual( values = c( "red" ), 
    labels = c( paste( "k=10; AUC=", round( auc( roc_knn_10_trans ), 3 ) )
                ) ) +
    theme( legend.title = element_blank() ) +
    theme( legend.position = c( .7, .3 ), text = element_text( size = 17 ) )

```
  
The area under the curve is 0.912.
  
</details>

<details class="sub_details">
  <summary>Comparison</summary>

In the following comparison we refer to the model where all 19 independent variables are included as "complete", and to the other model where `account.length`, `voice.plan`, `voice.messages`, `intl.plan`, `intl.mins`, `day.mins`, `eve.mins`, `night.mins`, and `customer.calls` included as "restricted" model.


| | Complete | Restricted |
| :- | :-: | :-: |
|MSE | 0.1457 | 0.0988 |
|Accuracy |0.8543 | 0.9012|
|Misclassification rate | 0.1457 | 0.0997 |
|Sensitivity |0.0193 | 0.4000 |
|Specificity |1.000 | 0.9887 |
|AUC | 0.3560 | 0.9120 |

Based on the above summary we can conclude the following:

* The restricted model bears with approximately 4.7% better error rate, than the complete model. That is, on average the restricted model is a better predictor, and is preferred over the complete model.
* The accuracy rate of the restricted model is higher by approximately 4.5%, and similary the misspecificaiton rate is lower by the same percentage for the restricted model.
* the restricted model shows significantly higher sensitivity rate than the complete model. That is, the restricted model predicts the positive records approximately 20 times better than the complete model. 
* In terms of specificity, the complete model performs better, however, the margin is about 1.2%, i.e., insiginificant.
* The area under the curve for the restricted model is almost 3 fold, therefore, it is a superior model to its competitor.

</details>


</details>

Compare the results with the previous section. What would be your conclusion? 

## Optimal value of k for the kNN algorithm

In the previous questions, to find the k-nearest neighbor for the test set, we set k = 10. But why 10? Find out the optimal value of `k`.

<details>
  <summary>Answer</summary>

```{r}  
kNN.plot( formula, train = train_set, test = test_set, transform = "minmax", 
          k.max = 30, set.seed = 13 )
```
  
To find the optimal value for k, we plot it the vlaue of k against the error rate the particlar k produces. We find, that the local minimum of the graph is for k = 8. Given the small error difference between the local minimum and the values in its neighbourhood, we may prefer a higher k value, for instance 9 or 10. 
  
</details>
   
# **Bonus**: Applying the kNN algorithm for your own dataset (30 points)

In this part, apply the kNN algorithm to analyze your own dataset. You could follow the same steps as in part 1 (above) of these exercises.

Note: to apply the kNN algorithm to your own dataset, the target variable has to be categorical, preferably Binary, similar to the above example.
  